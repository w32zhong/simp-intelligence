{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "009e4534-1b3f-40cc-ba0e-9d952992ddbc",
   "metadata": {},
   "source": [
    "Upon receiving any new request, either the [SGL.Engine](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/entrypoints/engine.py#L141-L292) entrypoint or the [launched server](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/entrypoints/http_server.py#L464-L499) entrypoint will call `scheduler.tokenizer_manager.generate_request`.\n",
    "\n",
    "The server entrypoint is just an HTTP server wrapper that calls [_launch_subprocesses](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/entrypoints/engine.py#L725-L872), same as what SGL.Engine does.\n",
    "So they will similarly \"launch\" two processes and a manager to host the background workers:\n",
    "1. [run_scheduler_process](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/entrypoints/engine.py#L775-L788) (using `mp.Process`):\n",
    "    1. [Scheduler](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/managers/scheduler.py#L2587)\n",
    "    2. [main event loop](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/managers/scheduler.py#L2612), it loops through:\n",
    "        1. [Receive the request](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/managers/scheduler.py#L811)\n",
    "           * from either (non-blocking) [tokenizer or rpc](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/managers/scheduler.py#L1018-L1025) zmq channels, depending on whether the caller is using SDK.\n",
    "        3. [Schedule and process the next batch](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/managers/scheduler.py#L814-L818)\n",
    "            * calling the [run_batch](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/managers/scheduler.py#L1798) function.\n",
    "        5. [Take the batch `result`](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/managers/scheduler.py#L819)\n",
    "            1. Depending on [decode/extend/prefill](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/managers/scheduler.py#L1881-L1884) decoding types, [stream out the results](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/managers/scheduler_output_processor_mixin.py#L303).\n",
    "            2. For finished request IDs, [relay results to detokenizer](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/managers/scheduler_output_processor_mixin.py#L716-L720) via zmq channel.\n",
    "3. [run_detokenizer_process](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/entrypoints/engine.py#L829-L835) (using `mp.Process`)\n",
    "    * [DetokenizerManager](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/managers/detokenizer_manager.py#L294-L298) init and event loop.\n",
    "       * Dispatch through [_request_dispatcher](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/managers/detokenizer_manager.py#L102-L120): `recv_from_scheduler` and then `send_to_tokenizer`.\n",
    "5. [_init_tokenizer_manager](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/entrypoints/engine.py#L844-L846)\n",
    "    * [TokenizerManager](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/managers/tokenizer_manager.py#L137)\n",
    "        * [generate_request](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/managers/tokenizer_manager.py#L330) (main callee by entrypoints)\n",
    "            * [auto_create_handle_loop](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/managers/tokenizer_manager.py#L1026)\n",
    "                * [handle_loop](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/managers/tokenizer_manager.py#L1177) routes requests to, e.g., `_handle_batch_output` (via `_result_dispatcher`) which eventually calls `state.event.set()` and log metrics.\n",
    "            * Do tokenization and [_send_one_request](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/managers/tokenizer_manager.py#L619) (`self.send_to_scheduler.send_pyobj(tokenized_obj)`) then `_wait_one_response`. Or,\n",
    "            * call [_handle_batch_request](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/managers/tokenizer_manager.py#L739)\n",
    "               * Under sequential tokenization and processing, invoke [the tokenization steps](https://github.com/sgl-project/sglang/blob/v0.5.2/python/sglang/srt/managers/tokenizer_manager.py#L775-L784) for each instance of the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4138c1f6-4e60-4291-ace0-3e2f7ba502e4",
   "metadata": {},
   "source": [
    "The TokenizerManager request origin:\n",
    "```py\n",
    "async def generate_request(\n",
    "        self,\n",
    "        obj: Union[GenerateReqInput, EmbeddingReqInput],\n",
    "        request: Optional[fastapi.Request] = None,\n",
    "    ):\n",
    "        self.auto_create_handle_loop()\n",
    "        ...\n",
    "        if self.server_args.enable_lora and obj.lora_path:\n",
    "                # Look up the LoRA ID from the registry and start tracking ongoing LoRA requests.\n",
    "                obj.lora_id = await self.lora_registry.acquire(obj.lora_path)\n",
    "\n",
    "            if obj.is_single:\n",
    "                tokenized_obj = await self._tokenize_one_request(obj)\n",
    "                state = self._send_one_request(obj, tokenized_obj, created_time)\n",
    "                async for response in self._wait_one_response(obj, state, request):\n",
    "                    yield response\n",
    "            else:\n",
    "                async for response in self._handle_batch_request(\n",
    "                    obj, request, created_time\n",
    "                ):\n",
    "                    yield response\n",
    "        \n",
    "def _send_one_request(\n",
    "        self,\n",
    "        obj: Union[GenerateReqInput, EmbeddingReqInput],\n",
    "        tokenized_obj: Union[TokenizedGenerateReqInput, TokenizedEmbeddingReqInput],\n",
    "        created_time: Optional[float] = None,\n",
    "    ):\n",
    "        self.send_to_scheduler.send_pyobj(tokenized_obj)\n",
    "        state = ReqState([], False, asyncio.Event(), obj, created_time=created_time)\n",
    "        self.rid_to_state[obj.rid] = state\n",
    "        return state\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
