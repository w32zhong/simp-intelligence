{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eee8cb74-7188-46fe-a7af-0e761f6063ef",
   "metadata": {},
   "source": [
    "# Layers\n",
    "Each layer $f$ in a neural network is just a function mapping from $\\mathbb{R}^m \\rightarrow \\mathbb{R}^n $. \n",
    "\n",
    "## Chain Rule\n",
    "Without loss of generality, consider the scaler version $z(t) = f(x(t), y(t))$, we can show: \n",
    "\n",
    "$$ \\begin{aligned}  z'(t) =& \\lim_{dt \\to 0} \\frac{f(x(t+dt), y(t+dt)) - f(x(t), y(t))}{dt} \\\\\\\\  =& \\lim_{dt \\to 0} \\frac{  f(x(t+dt), y(t+dt)) - f(x(t+dt), y(t))  + f(x(t+dt), y(t)) - f(x(t), y(t))  }{dt} \\\\\\\\  =& \\lim_{dt \\to 0} \\frac{f(x(t+dt), y(t+dt)) - f(x(t+dt), y(t))}{dt}  + \\lim_{dt \\to 0} \\frac{f(x(t+dt), y(t)) - f(x(t), y(t))}{dt} \\\\\\\\  =& \\lim_{dt \\to 0} \\frac{f(x(t+dt), y(t+dt)) - f(x(t+dt), y(t))}  {y(t+dt) - y(t)} \\times  \\frac{y(t+dt) - y(t)}{dt} + \\\\\\\\  & \\lim_{dt \\to 0} \\frac{f(x(t+dt), y(t)) - f(x(t), y(t))}  {x(x+dt) - x(t)} \\times  \\frac{x(x+dt) - x(t)}{dt} \\\\\\\\  \\doteq& \\lim_{dt \\to 0} \\frac{f(x(t+dt), y(t) + \\Delta y) - f(x(t+dt), y(t))}  {\\Delta y} \\times  \\frac{y(t+dt) - y(t)}{dt} + \\\\\\\\  & \\lim_{dt \\to 0} \\frac{f(x(t) + \\Delta x, y(t)) - f(x(t), y(t))}  {\\Delta x} \\times  \\frac{x(x+dt) - x(t)}{dt} \\\\\\\\  =& \\left.\\frac{\\partial f}{\\partial y}\\right|_{y=y(t)} \\cdot \\frac{\\partial y}{\\partial t}  + \\left.\\frac{\\partial f}{\\partial x}\\right|_{x=x(t)} \\cdot \\frac{\\partial x}{\\partial t} \\end{aligned} $$ \n",
    "\n",
    "iff $dt \\rightarrow 0$ implies $\\Delta x \\rightarrow 0$ and $\\Delta y \\rightarrow 0$ (Lipschitz continuity). \n",
    "\n",
    "In more general case when $z(t) = f(x(t))$ where $x \\in \\mathbb{R}^n, t \\in \\mathbb{R}^m, f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ and $x: \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$, \n",
    "\n",
    "$$ \\begin{aligned} \\frac{\\partial z}{\\partial t_i} =& \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} & ... & \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}_{x = x(t)} \\cdot \\begin{bmatrix} \\frac{\\partial x_1}{\\partial t_i} \\\\\\\\ \\vdots \\\\\\\\ \\frac{\\partial x_n}{\\partial t_i} \\end{bmatrix} \\\\\\\\ \\doteq& \\nabla_x^T f (x = x(t)) \\cdot \\begin{bmatrix} \\frac{\\partial x_1}{\\partial t_i} \\\\\\\\ \\vdots \\\\\\\\ \\frac{\\partial x_n}{\\partial t_i} \\end{bmatrix} \\\\\\\\ \\end{aligned} $$ \n",
    "\n",
    "therefore \n",
    "\n",
    "$$ \\tag{1} \\nabla_t^T z(t) \\doteq \\begin{bmatrix} \\frac{\\partial f}{\\partial t_1}, ..., \\frac{\\partial f}{\\partial t_m} \\end{bmatrix} = \\nabla_x^T f (x = x(t)) \\cdot \\begin{bmatrix}  \\partial x_1 / \\partial t_1 & \\partial x_1 / \\partial t_2 & ... & \\partial x_1 / \\partial t_m \\\\\\\\  \\partial x_2 / \\partial t_1 & \\partial x_2 / \\partial t_2 & ... & \\partial x_2 / \\partial t_m \\\\\\\\  \\vdots & \\ddots \\\\\\\\  \\partial x_n / \\partial t_1 & \\partial x_n / \\partial t_2 & ... & \\partial x_n / \\partial t_m \\\\\\\\ \\end{bmatrix} $$ \n",
    "\n",
    "where the RHS matrix is called the Jacobian matrix $J_t x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f69a3a-ae1e-4776-a97e-effe8a6150ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAX (Conda ENV Mojo kernel)",
   "language": "mojo",
   "name": "mojo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "mojo"
   },
   "file_extension": ".mojo",
   "mimetype": "text/x-mojo",
   "name": "mojo"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
