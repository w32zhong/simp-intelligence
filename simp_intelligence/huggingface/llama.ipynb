{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2766264-4ecc-4e70-9749-7e9d1a6830ca",
   "metadata": {},
   "source": [
    "This is a reference code for the Llama Model from a old HuggingFace implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "720bdbb1-67d3-4aed-bcf1-48642ca7d616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tk/.local/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "def _make_causal_mask(bsz, seq_len, past_seq_len, dtype, device):\n",
    "    # make an infinity square-matrix mask\n",
    "    mask = torch.full(\n",
    "        (seq_len, seq_len),\n",
    "        torch.tensor(torch.finfo(dtype).min, device=device),\n",
    "    device=device)\n",
    "    # fill lower diagonal by zeros\n",
    "    mask_cond = torch.arange(mask.size(-1), device=device)\n",
    "    inverted_mask_cond = (mask_cond + 1).view(mask.size(-1), 1)\n",
    "    mask.masked_fill_(mask_cond < inverted_mask_cond, 0)\n",
    "    mask = mask.to(dtype)\n",
    "    # concatenate previous mask to a potentially rectangular mask\n",
    "    if past_seq_len > 0:\n",
    "        prev_mask = torch.zeros(seq_len, past_seq_len,\n",
    "            dtype=dtype, device=device)\n",
    "        mask = torch.cat([prev_mask, mask], dim=-1)\n",
    "    # Expanding a tensor to a desired dim. This does not allocate\n",
    "    # new memory, but only creates a new view on the existing tensor.\n",
    "    return mask[None, None, :, :].expand(\n",
    "        bsz, 1, seq_len, seq_len + past_seq_len)\n",
    "\n",
    "\n",
    "def _expand_mask(mask, dtype, seq_len):\n",
    "    bsz, src_len = mask.size()\n",
    "    expanded_mask = mask[:, None, None, :].expand(\n",
    "        bsz, 1, seq_len, src_len).to(dtype)\n",
    "    inverted_mask = 1.0 - expanded_mask\n",
    "    return inverted_mask.masked_fill(\n",
    "        inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
    "\n",
    "\n",
    "class LlamaRMSNorm(DistributedModule):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = DistributedParameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, states):\n",
    "        input_dtype = states.dtype\n",
    "        variance = states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n",
    "        rsqrt = torch.rsqrt(variance + self.variance_epsilon)\n",
    "        states = states * rsqrt\n",
    "        return (self.weight * states).to(input_dtype)\n",
    "\n",
    "\n",
    "class LlamaRotaryEmbedding(DistributedModule):\n",
    "    def __init__(self, dim,\n",
    "        max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        theta = 1.0 / (base ** (\n",
    "            torch.arange(0, dim, 2).float().to(device) / dim)\n",
    "        )\n",
    "        # theta: [head_H // 2]\n",
    "\n",
    "        # Buffer will show up in state_dict(), no need to save it here.\n",
    "        # self.register_buffer(\"theta\", theta, persistent=False)\n",
    "\n",
    "        t = torch.arange(max_position_embeddings,\n",
    "            device=theta.device, dtype=theta.dtype)\n",
    "        # t: [max_seq_len]\n",
    "\n",
    "        cache = torch.einsum(\"i,j->ij\", t, theta)\n",
    "        # cache_{i,j} = t_i * theta_j\n",
    "        # cache: [max_seq_len, head_H // 2]\n",
    "\n",
    "        cache = torch.cat((cache, cache), dim=-1)\n",
    "        # cache: [max_seq_len, head_H]\n",
    "\n",
    "        dtype = torch.get_default_dtype()\n",
    "        self.cos_cached = cache.cos().to(dtype)\n",
    "        self.sin_cached = cache.sin().to(dtype)\n",
    "        # {sin, cos}_cached: [max_seq_len, head_H]\n",
    "\n",
    "    def forward(self, tot_seq_len):\n",
    "        # get partial cache that matches the input tot_seq_len\n",
    "        return (\n",
    "            self.cos_cached[:tot_seq_len, ...],\n",
    "            self.sin_cached[:tot_seq_len, ...]\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def apply(q, k, cos, sin, position_ids, timestep=0):\n",
    "        cos = cos.to(device=q.device, dtype=q.dtype)\n",
    "        sin = sin.to(device=q.device, dtype=q.dtype)\n",
    "        # {sin, cos}: [tot_seq_len, head_H]\n",
    "\n",
    "        # position_ids: [bs, seq_len]\n",
    "        sin = sin[position_ids].unsqueeze(1)\n",
    "        cos = cos[position_ids].unsqueeze(1)\n",
    "        # Because position_ids are either full initial ids\n",
    "        # or the current decoding index, the {sin, cos} now\n",
    "        # becomes of partial tot_seq_len, i.e., seq_len:\n",
    "        # {sin, cos}: [bs, 1, seq_len, head_H]\n",
    "        # Now, {sin, cos} @ [bs, 1, m, k] is:\n",
    "        #   cos(m * theta_k) when k < head_H//2\n",
    "        #   cos(m * theta_{k - head_H//2}) when otherwise\n",
    "\n",
    "        # split the head_H dimension, so that\n",
    "        # x @ [bs, heads, m, :] is: [-x_m[mid:]; x_m[:mid]]\n",
    "        def rotate_half(x):\n",
    "            x1 = x[..., : x.shape[-1] // 2]\n",
    "            x2 = x[..., x.shape[-1] // 2 :]\n",
    "            return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "        # Now, apply RoPE using element-wise product...\n",
    "        # Different original author blog post, HuggingFace Llama\n",
    "        # implementation apply rotation on the 2-D slice\n",
    "        #   [x_{d}, x_{head_H//2 + d}]\n",
    "        # instead of\n",
    "        #   [x_{2d+0}, x_{2d+1}]\n",
    "\n",
    "        # given a 4-D vector x for example (x can be q or k):\n",
    "        #\n",
    "        # | cos(m θ_0) -sin(m θ_0)                      |   |x0|\n",
    "        # | sin(m θ_0)  cos(m θ_0)                      | * |x2|\n",
    "        # |                      cos(m θ_1) -sin(m θ_1) |   |x1|\n",
    "        # |                      sin(m θ_1)  cos(m θ_1) |   |x3|\n",
    "        #\n",
    "        # which equals\n",
    "        #\n",
    "        # | x0 cos(m θ_0) - x2 sin(m θ_0) |\n",
    "        # | x2 cos(m θ_0) + x0 sin(m θ_0) |\n",
    "        # | x1 cos(m θ_1) - x3 sin(m θ_1) |\n",
    "        # | x3 cos(m θ_1) + x1 sin(m θ_1) |\n",
    "        #\n",
    "        # and if we reorder it back, the transformed vector is:\n",
    "        #\n",
    "        # | x0 cos(m θ_0) - x2 sin(m θ_0) |\n",
    "        # | x1 cos(m θ_1) - x3 sin(m θ_1) |\n",
    "        # | x2 cos(m θ_0) + x0 sin(m θ_0) |\n",
    "        # | x3 cos(m θ_1) + x1 sin(m θ_1) |\n",
    "        #\n",
    "        # which is essentially what is written below:\n",
    "        q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "        k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "        return q_embed, k_embed\n",
    "\n",
    "\n",
    "class SiLUActivation(DistributedModule):\n",
    "    def forward(self, input):\n",
    "        return silu(input)\n",
    "\n",
    "\n",
    "class LlamaMLP(DistributedModule):\n",
    "    def __init__(self, hidden_size, intermediate_size):\n",
    "        super().__init__()\n",
    "        self.gate_proj = DistributedLinear(hidden_size, intermediate_size, bias=False)\n",
    "        self.down_proj = DistributedLinear(intermediate_size, hidden_size, bias=False)\n",
    "        self.up_proj = DistributedLinear(hidden_size, intermediate_size, bias=False)\n",
    "        self.act_fn = SiLUActivation()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # SwiGLU: https://arxiv.org/abs/2002.05202v1\n",
    "        return self.down_proj(\n",
    "            self.act_fn(self.gate_proj(x))  * self.up_proj(x)\n",
    "        )\n",
    "\n",
    "\n",
    "class LlamaAttention(DistributedModule):\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size # H\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads # head_H\n",
    "        H = self.num_heads * self.head_dim\n",
    "        self.q_proj = DistributedLinear(self.hidden_size, H, bias=False)\n",
    "        self.k_proj = DistributedLinear(self.hidden_size, H, bias=False)\n",
    "        self.v_proj = DistributedLinear(self.hidden_size, H, bias=False)\n",
    "        self.o_proj = DistributedLinear(H, self.hidden_size, bias=False)\n",
    "        self.rotary_emb = LlamaRotaryEmbedding(\n",
    "            self.head_dim,\n",
    "            max_position_embeddings=config.max_position_embeddings\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        past_cache=None,\n",
    "        use_cache=False,\n",
    "        timestep=0):\n",
    "        bsz, seq_len, _ = hidden_states.size()\n",
    "        # split inputs into heads of dimension [B, heads, seq_len, head_H]\n",
    "        split_dim = (bsz, seq_len, self.num_heads, self.head_dim)\n",
    "        Q = self.q_proj(hidden_states).view(*split_dim).transpose(1, 2)\n",
    "        K = self.k_proj(hidden_states).view(*split_dim).transpose(1, 2)\n",
    "        V = self.v_proj(hidden_states).view(*split_dim).transpose(1, 2)\n",
    "\n",
    "        tot_seq_len = seq_len\n",
    "        if past_cache is not None:\n",
    "            tot_seq_len += past_cache[0].shape[-2]\n",
    "\n",
    "        # get rotary position embedding\n",
    "        cos, sin = self.rotary_emb(tot_seq_len)\n",
    "        # cos, sin: [1, 1, tot_seq_len, head_H]\n",
    "        Q, K = LlamaRotaryEmbedding.apply(\n",
    "            Q, K, cos, sin, position_ids, timestep=timestep)\n",
    "        # Q, K: [B, heads, tot_seq_len, head_H]\n",
    "\n",
    "        if past_cache is not None:\n",
    "            # reuse past K, V, self_attention\n",
    "            past_key, past_val = past_cache\n",
    "            # past_key or past_val: [B, heads, past_seq_len, head_H]\n",
    "            K = torch.cat([past_key, K], dim=2)\n",
    "            V = torch.cat([past_val, V], dim=2)\n",
    "            # new K or V: [B, heads, tot_seq_len, head_H]\n",
    "        # set either the initial or concatenated past_cache\n",
    "        past_cache = (K, V) if use_cache else None\n",
    "\n",
    "        # apply scaled dot-product self-attention\n",
    "        attn_W = torch.matmul(\n",
    "            Q, K.transpose(2, 3)\n",
    "        ) / math.sqrt(self.head_dim)\n",
    "        assert attn_W.size() == (bsz, self.num_heads, seq_len, tot_seq_len)\n",
    "\n",
    "        # apply attention_mask!\n",
    "        if attention_mask is not None:\n",
    "            assert attention_mask.size() == (bsz, 1, seq_len, tot_seq_len)\n",
    "            attn_W = attn_W + attention_mask\n",
    "\n",
    "        # upcast to fp32 before softmax and downcast back to the original dtype\n",
    "        attn_W = softmax(attn_W, dim=-1, dtype=torch.float32).to(Q.dtype)\n",
    "\n",
    "        # apply attention weights to Value\n",
    "        attn_out = torch.matmul(attn_W, V)\n",
    "        assert attn_out.size() == (bsz, self.num_heads, seq_len, self.head_dim)\n",
    "\n",
    "        # join heads\n",
    "        attn_out = attn_out.transpose(1, 2)\n",
    "        attn_out = attn_out.reshape(bsz, seq_len, self.hidden_size)\n",
    "\n",
    "        # attention output projection\n",
    "        attn_out = self.o_proj(attn_out)\n",
    "        return attn_out, past_cache\n",
    "\n",
    "\n",
    "class LlamaDecoderLayer(Module):\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.self_attn = LlamaAttention(config=config)\n",
    "        self.mlp = LlamaMLP(\n",
    "            hidden_size=self.hidden_size,\n",
    "            intermediate_size=config.intermediate_size,\n",
    "        )\n",
    "        self.norm1 = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.norm2 = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        past_cache=None,\n",
    "        use_cache=False,\n",
    "        timestep=0):\n",
    "\n",
    "        # Background: https://arxiv.org/pdf/2002.04745.pdf\n",
    "        #\n",
    "        # Original Post-LayerNorm         Pre-LayerNorm Layer\n",
    "        #\n",
    "        #        x(l+1)                       x(l+1)\n",
    "        #         |                            |\n",
    "        #      LayerNorm                      (+)----*\n",
    "        #         |                            |     |\n",
    "        #        (+)----*                     FFN    |\n",
    "        #         |     |                      |     |\n",
    "        #        FFN    |                LayerNorm   |\n",
    "        #         |     |                      |     |\n",
    "        #         *-----*                      *-----*\n",
    "        #         |                            |\n",
    "        #      LayerNorm                      (+)----*\n",
    "        #         |                            |     |\n",
    "        #        (+)----*                Attention   |\n",
    "        #         |     |                      |     |\n",
    "        #     Attention |                LayerNorm   |\n",
    "        #         |     |                      |     |\n",
    "        #         *-----*                      *-----*\n",
    "        #         |                            |\n",
    "        #        x(l)                         x(l)\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.norm1(hidden_states)\n",
    "        hidden_states, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_cache=past_cache,\n",
    "            use_cache=use_cache,\n",
    "            timestep=timestep\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.norm2(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "        if use_cache:\n",
    "            outputs += (present_key_value,)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class LlamaModel(Module):\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__()\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embed_tokens = DistributedEmbedding(\n",
    "            config.vocab_size, config.hidden_size, self.padding_idx\n",
    "        )\n",
    "        if distributed:\n",
    "            from bmtrain import CheckpointBlock\n",
    "            self.layers = ModuleList([\n",
    "                CheckpointBlock(LlamaDecoderLayer(config))\n",
    "                for _ in range(config.num_hidden_layers)\n",
    "            ])\n",
    "        else:\n",
    "            self.layers = ModuleList([\n",
    "                LlamaDecoderLayer(config)\n",
    "                for _ in range(config.num_hidden_layers)\n",
    "            ])\n",
    "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def _prepare_decoder_attention_mask(self,\n",
    "        attention_mask, static_embeds, past_seq_len):\n",
    "        bsz, seq_len, _ = static_embeds.shape\n",
    "        # make a rectangular causal mask with past history\n",
    "        combined_attention_mask = _make_causal_mask(\n",
    "            bsz, seq_len, past_seq_len,\n",
    "            static_embeds.dtype,\n",
    "            static_embeds.device,\n",
    "        )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # add (expanded) attention_mask to causal mask\n",
    "            expanded_mask = _expand_mask(\n",
    "                attention_mask,\n",
    "                static_embeds.dtype,\n",
    "                seq_len\n",
    "            ).to(static_embeds.device)\n",
    "            combined_attention_mask += expanded_mask\n",
    "\n",
    "        return combined_attention_mask\n",
    "\n",
    "    def forward(self, input_ids,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        past_caches=None,\n",
    "        use_cache=None,\n",
    "        timestep=0):\n",
    "        # calculate various lengths\n",
    "        batch_size, seq_length = input_ids.shape\n",
    "        tot_seq_len = seq_length\n",
    "        past_seq_len = 0\n",
    "        if past_caches is not None:\n",
    "            # past_caches[layer][k/v]:\n",
    "            # [B, heads, past_seq_len, head_H]\n",
    "            past_seq_len = past_caches[0][0].shape[2]\n",
    "            tot_seq_len += past_seq_len\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(\n",
    "                past_seq_len, tot_seq_len,\n",
    "                dtype=torch.long,\n",
    "                device=input_ids.device\n",
    "            )\n",
    "            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n",
    "        else:\n",
    "            position_ids = position_ids.view(-1, seq_length).long()\n",
    "        # position_ids: [B, seq_len] using this timestep as the start pos\n",
    "\n",
    "        # [B, seq_len, H] non-contextual word embeddings\n",
    "        static_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        assert attention_mask.shape == (batch_size, tot_seq_len)\n",
    "        # convert linear attention mask to causal (rectangular) attention\n",
    "        attention_mask = self._prepare_decoder_attention_mask(\n",
    "            attention_mask, # [B, tot_seq_len]\n",
    "            static_embeds, # [B, seq_len, H]\n",
    "            past_seq_len\n",
    "        )\n",
    "        assert attention_mask.shape == (batch_size, 1,\n",
    "            seq_length, tot_seq_len)\n",
    "        # Example causal attention_mask:\n",
    "        #\n",
    "        # Case 1 (when timestep = 0, tot_seq_len = seq_len):\n",
    "        # |  0  -inf ... -inf -inf -inf | (q_{t=0})\n",
    "        # |  0    0  ... -inf -inf -inf |\n",
    "        # ...\n",
    "        # |  0    0  ...   0    0  -inf |\n",
    "        # |  0    0  ...   0    0    0  | (q_{t=seq_len})\n",
    "        #\n",
    "        # Case 2 (when timestep > 0, tot_seq_len = past_seq_len + 1):\n",
    "        # |  0    0  ...   0    0    0   0 | (all zeros)\n",
    "\n",
    "        # decoder layers\n",
    "        hidden_states = static_embeds # [B, seq_len, H]\n",
    "        new_caches = () if use_cache else None\n",
    "        for idx, decoder_layer in enumerate(self.layers):\n",
    "            # get \"past_cache\" at this idx-th layer\n",
    "            past_cache = (past_caches[idx]\n",
    "                if past_caches is not None else None)\n",
    "\n",
    "            # layer_outputs = (hidden_states, layer_cache)\n",
    "            #   hidden_states: [B, seq_len, H]\n",
    "            #   layer_cache = (K, V)\n",
    "            #     K: [B, heads, tot_seq_len, head_H]\n",
    "            #     V: [B, heads, tot_seq_len, head_H]\n",
    "            layer_outputs = decoder_layer(\n",
    "                hidden_states,                 # [B, seq_len, H]\n",
    "                attention_mask=attention_mask, # [B, 1, seq_len, tot_seq_len]\n",
    "                position_ids=position_ids,     # [B, seq_len]\n",
    "                past_cache=past_cache,     # [B, heads, past_seq_len, head_H]\n",
    "                use_cache=use_cache,\n",
    "                timestep=timestep\n",
    "            )\n",
    "            hidden_states = layer_outputs[0] # next recurrent states\n",
    "            if use_cache:\n",
    "                layer_cache = layer_outputs[1]\n",
    "                new_caches += (layer_cache,)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        return hidden_states, new_caches\n",
    "\n",
    "\n",
    "class LlamaForCausalLM(Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config)\n",
    "        self.lm_head = DistributedLinear(\n",
    "            config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        past_caches=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        timestep=0):\n",
    "        # invoke Llama model\n",
    "        hidden_states, new_caches = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_caches=past_caches,\n",
    "            use_cache=use_cache,\n",
    "            timestep=timestep\n",
    "        )\n",
    "        # convert to sparse logits\n",
    "        logits = self.lm_head(hidden_states) # [B, seq_len, vocab]\n",
    "        return logits, new_caches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
