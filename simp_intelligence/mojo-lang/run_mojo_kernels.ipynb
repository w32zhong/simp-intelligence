{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd68166-a7b7-41fd-bbc1-061ab1b3f3ef",
   "metadata": {},
   "source": [
    "## Load Mojo Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec5e69b8-e14d-4e77-9965-0979d23c8160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy, torch\n",
    "from pathlib import Path\n",
    "from max.torch import CustomOpLibrary\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "op_dir = os.path.abspath('operations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99273aef-9df8-4654-9daf-90f9b4c0174a",
   "metadata": {},
   "source": [
    "## Simple `add_one` Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd917d2-a609-463a-806a-604a531b254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "op_lib = CustomOpLibrary(Path(op_dir))\n",
    "add_one = op_lib.my_add_constant[{\"value\": 1}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f452223-31b4-47b4-9393-d594edfe904c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch_add_one cpu tensor([1001., 1001., 1001.,  ..., 1001., 1001., 1001.]) 0.004054727032780647\n",
      "mojo_add_one cpu tensor([1001., 1001., 1001.,  ..., 1001., 1001., 1001.]) 0.14450858801137656\n",
      "torch_add_one cuda tensor([1001., 1001., 1001.,  ..., 1001., 1001., 1001.], device='cuda:0') 0.007304457016289234\n",
      "mojo_add_one cuda tensor([1001., 1001., 1001.,  ..., 1001., 1001., 1001.], device='cuda:0') 0.11011582997161895\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def torch_add_one(inputs):\n",
    "    return inputs + 1\n",
    "\n",
    "def mojo_add_one(inputs):\n",
    "    outputs = torch.zeros_like(inputs)\n",
    "    add_one(outputs, inputs)\n",
    "    return outputs\n",
    "\n",
    "for device in [\"cpu\", \"cuda\"]:\n",
    "    for op in [torch_add_one, mojo_add_one]:\n",
    "        x = torch.zeros(1024, device=device)\n",
    "        x = op(x) # warm-up\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(1000):\n",
    "            x = op(x)\n",
    "        end = time.perf_counter()\n",
    "        print(op.__name__, device, x, end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea389f7b-411d-4e4a-92f9-8b9bd55c7213",
   "metadata": {},
   "source": [
    "## Different MatMul Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab358e3e-c603-4d58-bbe3-280dbb41d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from max.driver import Accelerator, accelerator_count, Tensor\n",
    "import torch\n",
    "M, K, N = 8, 8, 16 # debug\n",
    "M, K, N = 4096, 6144, 2048 # real-world scale\n",
    "device = Accelerator()\n",
    "torch_A = torch.randn(M, K)\n",
    "torch_B = torch.randn(K, N)\n",
    "torch_result = (torch_A @ torch_B).detach().cpu().numpy()\n",
    "A = Tensor.from_numpy(torch_A.numpy()).to(device)\n",
    "B = Tensor.from_numpy(torch_B.numpy()).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b255918a-cd9f-47a6-afe3-798e27f65945",
   "metadata": {},
   "source": [
    "Build and test executing the CUDA graph for our MatMul kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4827f036-dd78-4a55-af95-3799b6574352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building cuda graph for tensor_core_matmul\n",
      "loading cuda graph...\n",
      "test run:\n",
      " [[ -72.80523      43.381332     -0.48134637 ...    0.5543761\n",
      "    57.04894     -37.91275   ]\n",
      " [  33.03406     -26.666258     35.28915    ...  210.54146\n",
      "  -114.84794      -6.355719  ]\n",
      " [-102.443634   -119.57497    -110.37925    ...   11.614085\n",
      "    73.72262     -20.905973  ]\n",
      " ...\n",
      " [-151.94147      -5.1379514    -3.2820497  ...   57.451336\n",
      "    14.925808     65.16257   ]\n",
      " [ -28.677015    -31.575888     30.7144     ...   17.16105\n",
      "    40.508167    -76.59762   ]\n",
      " [  88.376976    -30.413141   -110.30627    ...    4.386332\n",
      "   105.50478       5.332529  ]]\n",
      "\n",
      "reference:\n",
      " [[ -72.85087      43.41088      -0.4847288  ...    0.59809685\n",
      "    57.12476     -37.933838  ]\n",
      " [  33.067142    -26.697542     35.307568   ...  210.66296\n",
      "  -114.961754     -6.3434296 ]\n",
      " [-102.53347    -119.66125    -110.51503    ...   11.600237\n",
      "    73.74789     -20.949192  ]\n",
      " ...\n",
      " [-152.0698       -5.1372375    -3.2726097  ...   57.500652\n",
      "    14.920652     65.20974   ]\n",
      " [ -28.725674    -31.591389     30.737253   ...   17.152964\n",
      "    40.562828    -76.63894   ]\n",
      " [  88.43853     -30.422142   -110.396736   ...    4.3700056\n",
      "   105.60575       5.3353815 ]]\n"
     ]
    }
   ],
   "source": [
    "from max.graph import Graph, TensorType, DeviceRef, ops\n",
    "def build_graph(session, algorithm):\n",
    "    print('building cuda graph for', algorithm)\n",
    "    with Graph(\"matmul_graph\",\n",
    "               input_types=[\n",
    "                   TensorType(dtype=A.dtype, shape=A.shape, device=DeviceRef.from_device(device)),\n",
    "                   TensorType(dtype=B.dtype, shape=B.shape, device=DeviceRef.from_device(device))\n",
    "               ],\n",
    "               custom_extensions=[Path(op_dir)]) as graph:\n",
    "        A_value, B_value = graph.inputs\n",
    "        output = ops.custom(\n",
    "            name=\"my_matmul\",\n",
    "            device=DeviceRef.from_device(device),\n",
    "            values=[A_value, B_value],\n",
    "            out_types=[\n",
    "                TensorType(dtype=A.dtype, shape=[\n",
    "                        A_value.tensor.shape[0], B_value.tensor.shape[1]\n",
    "                    ], device=DeviceRef.from_device(device))\n",
    "            ],\n",
    "            parameters={\"algorithm\": algorithm},\n",
    "        )\n",
    "        graph.output(output[0].tensor)\n",
    "    print('loading cuda graph...')\n",
    "    return session.load(graph) # compile the graph\n",
    "\n",
    "from max.engine import InferenceSession\n",
    "session = InferenceSession(devices=[device])\n",
    "graph =  build_graph(session, \"tensor_core_matmul\") # Change this to test a different algorithm\n",
    "mojo_result = graph.execute(A, B)[0].to_numpy()\n",
    "print(\"test run:\\n\", mojo_result, end=\"\\n\\n\")\n",
    "print(\"reference:\\n\", torch_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa076d-cc8f-4419-ae44-898598e2bade",
   "metadata": {},
   "source": [
    "Verify kernel results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e00e1bfc-e56e-4de5-873d-295e4784d8fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all matched!\n"
     ]
    }
   ],
   "source": [
    "if not numpy.allclose(mojo_result, torch_result, rtol=0, atol=0.5):\n",
    "    for row in range(torch_result.shape[0]):\n",
    "        if numpy.allclose(torch_result[row], mojo_result[row], rtol=0, atol=1.0): continue\n",
    "        print('obviously mismatch row:', row, 'delta:', numpy.absolute(torch_result[row] - mojo_result[row]).max())\n",
    "        print(torch_result[row])\n",
    "        print(mojo_result[row])\n",
    "    else:\n",
    "        print('almost matched!')\n",
    "else:\n",
    "    print('all matched!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f816fcb-554f-4d43-93de-93f69485c542",
   "metadata": {},
   "source": [
    "The tiled kernels can be visualized using the [matmul_visualization.mojo](./matmul_visualization.mojo) and `matmul_visualization_gui/*` scripts.\n",
    "\n",
    "Here are some example screenshots:\n",
    "![](./block_tiled_matrix_multiplication.png) \n",
    "![](./tensor_core_matmul_kernel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a96207d-db93-4a13-b238-e254741e6175",
   "metadata": {},
   "source": [
    "Run a complete benchmark for different algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c3014ac-d048-4ede-9e64-ba997362f2a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building cuda graph for naive\n",
      "loading cuda graph...\n",
      "naive {'torch': 0.019287150795571507, 'mojo': 1.0276804403867572}\n",
      "building cuda graph for coalescing\n",
      "loading cuda graph...\n",
      "coalescing {'torch': 0.01778892583679408, 'mojo': 0.13523418738041074}\n",
      "building cuda graph for tiled\n",
      "loading cuda graph...\n",
      "tiled {'torch': 0.018305375589989126, 'mojo': 0.1113776879850775}\n",
      "building cuda graph for tiled_register\n",
      "loading cuda graph...\n",
      "tiled_register {'torch': 0.01800552138593048, 'mojo': 0.4363975091837347}\n",
      "building cuda graph for block_tiling\n",
      "loading cuda graph...\n",
      "block_tiling {'torch': 0.017901817173697057, 'mojo': 0.025444415980018675}\n",
      "building cuda graph for block_tiling_vectorized\n",
      "loading cuda graph...\n",
      "block_tiling_vectorized {'torch': 0.01904884921386838, 'mojo': 0.03678875456098467}\n",
      "building cuda graph for tensor_core_matmul\n",
      "loading cuda graph...\n",
      "tensor_core_matmul {'torch': 0.01817250440362841, 'mojo': 0.14949885180685668}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for algo in [\"naive\", \"coalescing\", \"tiled\", \"tiled_register\", \"block_tiling\", \"block_tiling_vectorized\", \"tensor_core_matmul\"]:\n",
    "    graph =  build_graph(session, algo)\n",
    "    record = dict(torch=0, mojo=0)\n",
    "    sampels = 5\n",
    "    for _ in range(sampels):\n",
    "        torch_A = torch.randn(M, K).to('cuda:0')\n",
    "        torch_B = torch.randn(K, N).to('cuda:0')\n",
    "        A = Tensor.from_numpy(torch_A.cpu().numpy()).to(device)\n",
    "        B = Tensor.from_numpy(torch_B.cpu().numpy()).to(device)\n",
    "        # torch\n",
    "        torch.cuda.synchronize()\n",
    "        begin = time.perf_counter()\n",
    "        torch_result = (torch_A @ torch_B).detach().cpu().numpy()\n",
    "        torch.cuda.synchronize()\n",
    "        record['torch'] += (time.perf_counter() - begin) / sampels\n",
    "        # mojo\n",
    "        torch.cuda.synchronize()\n",
    "        begin = time.perf_counter()\n",
    "        mojo_result = graph.execute(A, B)\n",
    "        torch.cuda.synchronize()\n",
    "        record['mojo'] += (time.perf_counter() - begin) / sampels\n",
    "        assert numpy.allclose(mojo_result[0].to_numpy(), torch_result, rtol=0, atol=1.0)\n",
    "    print(algo, record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100c23ac-7763-4f1c-a179-154e4f1b8fdf",
   "metadata": {},
   "source": [
    "## Reference\n",
    "[1] https://github.com/modular/modular/blob/main/examples/custom_ops/kernels/matrix_multiplication.mojo\n",
    "\n",
    "[2] https://docs.modular.com/max/tutorials/custom-ops-matmul\n",
    "\n",
    "[3] Tensor Core Intro: https://www.nvidia.com/en-us/on-demand/session/gtcsj20-s21745/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
