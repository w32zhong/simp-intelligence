{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acd912d8-6169-4402-b4ee-683789abf437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.runtime import driver\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "\n",
    "def is_hip():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n",
    "\n",
    "\n",
    "def is_cdna():\n",
    "    return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',\n",
    "                                                                                   'gfx90a', 'gfx908')\n",
    "\n",
    "\n",
    "def naive_softmax(x):\n",
    "    \"\"\"Compute row-wise softmax of X using native pytorch\n",
    "\n",
    "    We subtract the maximum element in order to avoid overflows. Softmax is invariant to\n",
    "    this shift.\n",
    "    \"\"\"\n",
    "    # read  MN elements ; write M  elements\n",
    "    x_max = x.max(dim=1)[0]\n",
    "    # read MN + M elements ; write MN elements\n",
    "    z = x - x_max[:, None]\n",
    "    # read  MN elements ; write MN elements\n",
    "    numerator = torch.exp(z)\n",
    "    # read  MN elements ; write M  elements\n",
    "    denominator = numerator.sum(dim=1)\n",
    "    # read MN + M elements ; write MN elements\n",
    "    ret = numerator / denominator[:, None]\n",
    "    # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "186e82f7-82d5-425d-bd49-4141796984cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,\n",
    "                   num_stages: tl.constexpr):\n",
    "    # starting row of the program\n",
    "    row_start = tl.program_id(0)\n",
    "    row_step = tl.num_programs(0)\n",
    "    for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):\n",
    "        # The stride represents how much we need to increase the pointer to advance 1 row\n",
    "        row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "        # The block size is the next power of two greater than n_cols, so we can fit each\n",
    "        # row in a single block\n",
    "        col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "        input_ptrs = row_start_ptr + col_offsets\n",
    "        # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n",
    "        mask = col_offsets < n_cols\n",
    "        row = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n",
    "        # Subtract maximum for numerical stability\n",
    "        row_minus_max = row - tl.max(row, axis=0)\n",
    "        # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n",
    "        numerator = tl.exp(row_minus_max)\n",
    "        denominator = tl.sum(numerator, axis=0)\n",
    "        softmax_output = numerator / denominator\n",
    "        # Write back output to DRAM\n",
    "        output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "        output_ptrs = output_row_start_ptr + col_offsets\n",
    "        tl.store(output_ptrs, softmax_output, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc3dcb69-44d5-4133-95b9-cf5bf3d0243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = driver.active.utils.get_device_properties(DEVICE.index)\n",
    "NUM_SM = properties[\"multiprocessor_count\"]\n",
    "NUM_REGS = properties[\"max_num_regs\"]\n",
    "SIZE_SMEM = properties[\"max_shared_mem\"]\n",
    "WARP_SIZE = properties[\"warpSize\"]\n",
    "target = triton.runtime.driver.active.get_current_target()\n",
    "kernels = {}\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    n_rows, n_cols = x.shape\n",
    "\n",
    "    # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "\n",
    "    # Another trick we can use is to ask the compiler to use more threads per row by\n",
    "    # increasing the number of warps (`num_warps`) over which each row is distributed.\n",
    "    # You will see in the next tutorial how to auto-tune this value in a more natural\n",
    "    # way so you don't have to come up with manual heuristics yourself.\n",
    "    num_warps = 8\n",
    "\n",
    "    # Number of software pipelining stages.\n",
    "    num_stages = 4 if SIZE_SMEM > 200000 else 2\n",
    "\n",
    "    # Allocate output\n",
    "    y = torch.empty_like(x)\n",
    "\n",
    "    # pre-compile kernel to get register usage and compute thread occupancy.\n",
    "    kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,\n",
    "                                   num_stages=num_stages, num_warps=num_warps, grid=(1, ))\n",
    "    kernel._init_handles()\n",
    "    n_regs = kernel.n_regs\n",
    "    size_smem = kernel.metadata.shared\n",
    "    if is_hip():\n",
    "        # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.\n",
    "        # However, this is not always the case. In most cases all registers can be used as regular purpose registers.\n",
    "        # ISA SECTION (3.6.4 for CDNA3)\n",
    "        # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used\n",
    "        # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total\n",
    "        # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is\n",
    "        # not required to be equal numbers of both types.\n",
    "        NUM_GPRS = NUM_REGS\n",
    "        if is_cdna():\n",
    "            NUM_GPRS = NUM_REGS * 2\n",
    "\n",
    "        # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.\n",
    "        # When we divide this number with WARP_SIZE we get maximum number of waves that can\n",
    "        # execute on a CU (multi-processor)  in parallel.\n",
    "        MAX_NUM_THREADS = properties[\"max_threads_per_sm\"]\n",
    "        max_num_waves = MAX_NUM_THREADS // WARP_SIZE\n",
    "        occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps\n",
    "    else:\n",
    "        occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)\n",
    "    occupancy = min(occupancy, SIZE_SMEM // size_smem)\n",
    "    num_programs = NUM_SM * occupancy\n",
    "\n",
    "    num_programs = min(num_programs, n_rows)\n",
    "\n",
    "    # Create a number of persistent programs.\n",
    "    kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf2f72fc-f99b-43ec-8ec7-c089fe3e2946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/tmpephd8w09/__triton_launcher.c:3:10: fatal error: cuda.h: No such file or directory\n",
      "    3 | #include \"cuda.h\"\n",
      "      |          ^~~~~~~~\n",
      "compilation terminated.\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['/home/tk/anaconda3/bin/x86_64-conda-linux-gnu-cc', '/tmp/tmpephd8w09/__triton_launcher.c', '-O3', '-shared', '-fPIC', '-Wno-psabi', '-o', '/tmp/tmpephd8w09/__triton_launcher.cpython-312-x86_64-linux-gnu.so', '-lcuda', '-L/home/tk/Desktop/jupyter/simp-intelligence/.pixi/envs/default/lib/python3.12/site-packages/triton/backends/nvidia/lib', '-L/usr/lib', '-L/usr/lib32', '-I/home/tk/Desktop/jupyter/simp-intelligence/.pixi/envs/default/lib/python3.12/site-packages/triton/backends/nvidia/include', '-I/tmp/tmpephd8w09', '-I/home/tk/Desktop/jupyter/simp-intelligence/.pixi/envs/default/include/python3.12', '-I/home/tk/Desktop/jupyter/simp-intelligence/.pixi/envs/default/targets/x86_64-linux/include']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m torch.manual_seed(\u001b[32m0\u001b[39m)\n\u001b[32m      2\u001b[39m x = torch.randn(\u001b[32m1823\u001b[39m, \u001b[32m781\u001b[39m, device=DEVICE)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m y_triton = \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m y_torch = torch.softmax(x, axis=\u001b[32m1\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m torch.allclose(y_triton, y_torch), (y_triton, y_torch)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36msoftmax\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# pre-compile kernel to get register usage and compute thread occupancy.\u001b[39;00m\n\u001b[32m     29\u001b[39m kernel = softmax_kernel.warmup(y, x, x.stride(\u001b[32m0\u001b[39m), y.stride(\u001b[32m0\u001b[39m), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,\n\u001b[32m     30\u001b[39m                                num_stages=num_stages, num_warps=num_warps, grid=(\u001b[32m1\u001b[39m, ))\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[43mkernel\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_init_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m n_regs = kernel.n_regs\n\u001b[32m     33\u001b[39m size_smem = kernel.metadata.shared\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/jupyter/simp-intelligence/.pixi/envs/default/lib/python3.12/site-packages/triton/compiler/compiler.py:479\u001b[39m, in \u001b[36mCompiledKernel._init_handles\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    477\u001b[39m device = driver.active.get_current_device()\n\u001b[32m    478\u001b[39m \u001b[38;5;66;03m# create launcher\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m \u001b[38;5;28mself\u001b[39m.run = \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mactive\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlauncher_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# not enough shared memory to run the kernel\u001b[39;00m\n\u001b[32m    481\u001b[39m max_shared = max_shared_mem(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/jupyter/simp-intelligence/.pixi/envs/default/lib/python3.12/site-packages/triton/backends/nvidia/driver.py:686\u001b[39m, in \u001b[36mCudaLauncher.__init__\u001b[39m\u001b[34m(self, src, metadata)\u001b[39m\n\u001b[32m    684\u001b[39m tensordesc_meta = \u001b[38;5;28mgetattr\u001b[39m(metadata, \u001b[33m\"\u001b[39m\u001b[33mtensordesc_meta\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    685\u001b[39m src = make_launcher(constants, signature, tensordesc_meta)\n\u001b[32m--> \u001b[39m\u001b[32m686\u001b[39m mod = \u001b[43mcompile_module_from_src\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m__triton_launcher\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlibrary_dirs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_dirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_dirs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_dirs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlibraries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibraries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    693\u001b[39m has_tensor_desc_arg = \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(sig, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m sig.startswith(\u001b[33m\"\u001b[39m\u001b[33mtensordesc\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m sig \u001b[38;5;129;01min\u001b[39;00m signature.values())\n\u001b[32m    695\u001b[39m \u001b[38;5;28mself\u001b[39m.num_ctas = functools.reduce(operator.mul, metadata.cluster_dims, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/jupyter/simp-intelligence/.pixi/envs/default/lib/python3.12/site-packages/triton/runtime/build.py:110\u001b[39m, in \u001b[36mcompile_module_from_src\u001b[39m\u001b[34m(src, name, library_dirs, include_dirs, libraries)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(src_path, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    109\u001b[39m     f.write(src)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m so = \u001b[43m_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmpdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlibrary_dirs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_dirs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlibraries\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(so, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    112\u001b[39m     cache_path = cache.put(f.read(), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, binary=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/jupyter/simp-intelligence/.pixi/envs/default/lib/python3.12/site-packages/triton/runtime/build.py:73\u001b[39m, in \u001b[36m_build\u001b[39m\u001b[34m(name, src, srcdir, library_dirs, include_dirs, libraries)\u001b[39m\n\u001b[32m     71\u001b[39m cc_cmd += [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m-L\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mdir\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;129;01min\u001b[39;00m library_dirs]\n\u001b[32m     72\u001b[39m cc_cmd += [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m-I\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mdir\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;129;01min\u001b[39;00m include_dirs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcc_cmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDEVNULL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m so\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/jupyter/simp-intelligence/.pixi/envs/default/lib/python3.12/subprocess.py:413\u001b[39m, in \u001b[36mcheck_call\u001b[39m\u001b[34m(*popenargs, **kwargs)\u001b[39m\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cmd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    412\u001b[39m         cmd = popenargs[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, cmd)\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command '['/home/tk/anaconda3/bin/x86_64-conda-linux-gnu-cc', '/tmp/tmpephd8w09/__triton_launcher.c', '-O3', '-shared', '-fPIC', '-Wno-psabi', '-o', '/tmp/tmpephd8w09/__triton_launcher.cpython-312-x86_64-linux-gnu.so', '-lcuda', '-L/home/tk/Desktop/jupyter/simp-intelligence/.pixi/envs/default/lib/python3.12/site-packages/triton/backends/nvidia/lib', '-L/usr/lib', '-L/usr/lib32', '-I/home/tk/Desktop/jupyter/simp-intelligence/.pixi/envs/default/lib/python3.12/site-packages/triton/backends/nvidia/include', '-I/tmp/tmpephd8w09', '-I/home/tk/Desktop/jupyter/simp-intelligence/.pixi/envs/default/include/python3.12', '-I/home/tk/Desktop/jupyter/simp-intelligence/.pixi/envs/default/targets/x86_64-linux/include']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.randn(1823, 781, device=DEVICE)\n",
    "y_triton = softmax(x)\n",
    "y_torch = torch.softmax(x, axis=1)\n",
    "assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e83acd4-b4cb-4dcb-a806-38be9e766110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
