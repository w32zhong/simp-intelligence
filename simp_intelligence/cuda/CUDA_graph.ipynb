{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437701b4-925f-4f3e-845c-30f2e98014ff",
   "metadata": {},
   "source": [
    "## Summary\n",
    "CUDA Graph (proprietary) provides a mechanism to launch multiple GPU operations through a single CPU operation,\n",
    "and hence reduces the launching overheads between CUDA kernels. It\n",
    "1. capture (which puts a CUDA stream in capture mode) and \"build\" a static graph (static shapes and static control flow) on the first run, and\n",
    "2. replay or execute the graph subsequently.\n",
    "\n",
    "The difference is more pronounced when the same sequence of operations is repeated many times.\n",
    "Kernels in a replay also execute slightly faster on the GPU, but eliding CPU overhead is the main benefit.\n",
    "\n",
    "CUDA graph can be captured if it doesnâ€™t violate any of the following constraints:\n",
    "* Capture must occur on a non-default stream.\n",
    "* Ops that synchronize the CPU with the GPU (e.g., .item() calls) are prohibited.\n",
    "* Not using dynamic control flow, dynamic shapes (although we can interleave those code with `torch.cuda.make_graphed_callables` calls [1])\n",
    "* CUDA RNG operations are permitted, but they may require extra bookkeepings [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b19380-1c38-4f84-8252-f8f4cd84b2f0",
   "metadata": {},
   "source": [
    "## Example code [1]\n",
    "```python\n",
    "# Placeholders used for capture\n",
    "static_input = torch.randn(N, D_in, device='cuda')\n",
    "static_target = torch.randn(N, D_out, device='cuda')\n",
    "\n",
    "# capture\n",
    "g = torch.cuda.CUDAGraph()\n",
    "# Sets grads to None before capture, so backward() will create\n",
    "# .grad attributes with allocations from the graph's private pool\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "with torch.cuda.graph(g):\n",
    "    static_y_pred = model(static_input)\n",
    "    static_loss = loss_fn(static_y_pred, static_target)\n",
    "    static_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# replay\n",
    "for data, target in zip(real_inputs, real_targets):\n",
    "    static_input.copy_(data)\n",
    "    static_target.copy_(target)\n",
    "    # replay() includes forward, backward, and step.\n",
    "    # You don't even need to call optimizer.zero_grad() between iterations\n",
    "    # because the captured backward refills static .grad tensors in place.\n",
    "    g.replay()\n",
    "    # Params have been updated. static_y_pred, static_loss, and .grad\n",
    "    # attributes hold values from computing on this iteration's data.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa580b0-147c-4cb9-a549-2fd1ec07049e",
   "metadata": {},
   "source": [
    "## Reference\n",
    "[1] https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs\n",
    "\n",
    "[2] https://docs.pytorch.org/docs/main/notes/cuda.html#constraints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
