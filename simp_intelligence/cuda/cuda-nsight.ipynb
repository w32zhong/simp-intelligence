{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1099c281-9689-4755-9af7-e73200c653da",
   "metadata": {},
   "source": [
    "## Nsight Profilers\n",
    "Nvidia provides nsight-compute (`ncu`) and nsight-system (`nsys`) profilers. Alongside, it bundles with UI viewers (`ncu-ui` and `nsys-ui`).\n",
    "\n",
    "To use `ncu` **to generate any dump logs** without modifying nvidia system-wide module default configuration, you need superuser permission. \n",
    "\n",
    "Also, install the conda package `cuda` or its subset `nvidia::nsight-compute`.\n",
    "\n",
    "To profile, switch to the root account, and `conda activate` your environment, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f95caba3-c97e-4cce-91ab-a93933323b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for tk: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==PROF== Connected to process 401642 (/home/tk/Desktop/jupyter/simp-intelligence/.pixi/envs/default/bin/python3.13)\n",
      "==PROF== Profiling \"vectorized_elementwise_kernel\" - 0: 0%....50%....100% - 18 passes\n",
      "tensor([10], device='cuda:0')\n",
      "==PROF== Disconnected from process 401642\n",
      "==PROF== Report: /home/tk/Desktop/jupyter/simp-intelligence/simp_intelligence/cuda/test.ncu-rep\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo YOUR_ROOT_PASSWD_HERE | sudo -S ncu --set detailed -o test.ncu-rep -- python -c 'import torch; print(torch.tensor([1], device=\"cuda:0\") * 10)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492df9d-cd56-4b1f-8677-2fd5fc53fde8",
   "metadata": {},
   "source": [
    "If you ever see \n",
    "> ==ERROR== ERR_NVGPUCTRPERM - The user does not have permission to access NVIDIA GPU Performance Counters on the target device 0. For instructions on enabling permissions and to get more information see https://developer.nvidia.com/ERR_NVGPUCTRPERM\n",
    "\n",
    "it is because you have not configured the Nvidia system module to allow any user, and you are running as a non-root user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceb427c-fc45-4861-ae3a-bdf46083449b",
   "metadata": {},
   "source": [
    "When you use Huggingface Transformers, do not forget to redirect the model cache to the root environment:\n",
    "```bash\n",
    "export HF_HOME=/home/YOUR_USERNAME/.cache/huggingface\n",
    "export TRANSFORMERS_CACHE=$HF_HOME # for transformers < v4.0.0\n",
    "export HF_HUB_ENABLE_HF_TRANSFER=0\n",
    "```\n",
    "\n",
    "A one-liner to pass necessary environment variables to sudo:\n",
    "```bash\n",
    "sudo CUDA_VISIBLE_DEVICES=1 HF_HUB_ENABLE_HF_TRANSFER=0 HF_HOME=$HOME/.cache/huggingface \\\n",
    "    --preserve-env=LD_LIBRARY_PATH,PATH $(conda info --base)/bin/conda run -n YOUR_CONDA_ENV \\\n",
    "    --no-capture-output YOUR_COMMAND\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e96cb-9ad8-4768-9dcf-4239a460e27e",
   "metadata": {},
   "source": [
    "## Handle a real-world program with many kernel launches\n",
    "In the case of many AI applications, we want to skip many non-interesting stages such as \n",
    "the model load, initial warm-ups, etc.\n",
    "An example command is to attach the profiler to all processes, and start tracking a limited number of kernels (say 10) after skipping the initial a few hundreds of calls (say 300):\n",
    "```bash\n",
    "sudo CUDA_VISIBLE_DEVICES=1 HF_HUB_ENABLE_HF_TRANSFER=0 HF_HOME=$HOME/.cache/huggingface \\\n",
    "    --preserve-env=LD_LIBRARY_PATH,PATH $(conda info --base)/bin/conda run -n sglang \\\n",
    "    --no-capture-output  ncu --set detailed -f -o my_recap.dump --launch-skip 300 --launch-count 10 \\\n",
    "    --target-processes all -- python -m demo.sglang_inference engine_mode --dtype bfloat16 \\\n",
    "    --disable_cuda_graph --speculative_algorithm EAGLE w32zhong/laced-wood-90 --max_new_tokens 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d41c9-a90c-47fa-95f8-d4cff2396870",
   "metadata": {},
   "source": [
    "We can narrow down the profiling region and sample more accurately by calling the Nvidia NVTX API and using application-wise replay:\n",
    "```python\n",
    "if not getattr(self, 'flag', False):\n",
    "    torch.cuda.nvtx.range_push('draft')\n",
    "\n",
    "spec_info = self.draft(batch)\n",
    "\n",
    "if not getattr(self, 'flag', False):\n",
    "    torch.cuda.nvtx.range_pop()\n",
    "    self.flag = True\n",
    "```\n",
    "and trace only those kernels launched in between the \"draft\" range, and only for one time (using a manual Python variable `self.flag`):\n",
    "\n",
    "```bash\n",
    "ncu --set roofline -f -o 1layer.dump --nvtx --nvtx-include \"draft/\" --replay-mode application --target-processes all -- YOUR_COMMAND\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3b49a9-e54b-4c47-b67b-365723ad2605",
   "metadata": {},
   "source": [
    "## Misc.\n",
    "\n",
    "To see different log level in `ncu` (Metrics indicate the runtime costs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3edbbe43-b7a8-475b-9696-dae65863e81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- --------------------------------------------------------------------------- ------- -----------------\n",
      "Identifier Sections                                                                    Enabled Estimated Metrics\n",
      "---------- --------------------------------------------------------------------------- ------- -----------------\n",
      "basic      LaunchStats, Occupancy, SpeedOfLight, WorkloadDistribution                  yes     191              \n",
      "detailed   ComputeWorkloadAnalysis, LaunchStats, MemoryWorkloadAnalysis, MemoryWorkloa no      560              \n",
      "           dAnalysis_Chart, Occupancy, SourceCounters, SpeedOfLight, SpeedOfLight_Roof                          \n",
      "           lineChart, WorkloadDistribution                                                                      \n",
      "full       ComputeWorkloadAnalysis, InstructionStats, LaunchStats, MemoryWorkloadAnaly no      6581             \n",
      "           sis, MemoryWorkloadAnalysis_Chart, MemoryWorkloadAnalysis_Tables, NumaAffin                          \n",
      "           ity, Nvlink_Tables, Nvlink_Topology, Occupancy, PmSampling, SchedulerStats,                          \n",
      "            SourceCounters, SpeedOfLight, SpeedOfLight_HierarchicalDoubleRooflineChart                          \n",
      "           , SpeedOfLight_HierarchicalHalfRooflineChart, SpeedOfLight_HierarchicalSing                          \n",
      "           leRooflineChart, SpeedOfLight_HierarchicalTensorRooflineChart, SpeedOfLight                          \n",
      "           _RooflineChart, WarpStateStats, WorkloadDistribution                                                 \n",
      "nvlink     Nvlink, Nvlink_Tables, Nvlink_Topology                                      no      52               \n",
      "pmsampling PmSampling, PmSampling_WarpStates                                           no      231              \n",
      "roofline   SpeedOfLight, SpeedOfLight_HierarchicalDoubleRooflineChart, SpeedOfLight_Hi no      5912             \n",
      "           erarchicalHalfRooflineChart, SpeedOfLight_HierarchicalSingleRooflineChart,                           \n",
      "           SpeedOfLight_HierarchicalTensorRooflineChart, SpeedOfLight_RooflineChart, W                          \n",
      "           orkloadDistribution                                                                                  \n"
     ]
    }
   ],
   "source": [
    "!ncu --list-set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b185a52f-7be6-4e11-b0d6-bbd92d8b9799",
   "metadata": {},
   "source": [
    "For using ncu inside docker (also see ref 2):\n",
    "```bash\n",
    "$ systemctl isolate multi-user\n",
    "$ sudo modprobe -r nvidia_uvm nvidia_drm nvidia_modeset nvidia-vgpu-vfio nvidia\n",
    "$ sudo vim /etc/modprobe.d/nvidia-compute.conf  #  write `options nvidia \"NVreg_RestrictProfilingToAdminUsers=0\"` \n",
    "$ sudo update-initramfs -u\n",
    "$ sudo reboot\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f706eccf-8739-413f-b9d1-8ebe1677511b",
   "metadata": {},
   "source": [
    "## Reference\n",
    "1. https://developer.nvidia.com/nvidia-development-tools-solutions-err_nvgpuctrperm-permission-issue-performance-counters\n",
    "2. https://forums.developer.nvidia.com/t/use-nsight-compute-in-docker/73344/8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
