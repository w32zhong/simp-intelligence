{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa13f3a3-574e-4cd5-956a-28c33e5aa5d4",
   "metadata": {},
   "source": [
    "# Pytorch Backprop (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea98b97-f3f2-4c4a-a37c-ca3131e6546e",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "Let's first focus on the softmax function which is a $x \\to y: R^{n} \\rightarrow R^{n}$ mapping where any output element depends on all input elements, and its Jacobian matrix is somewhat non-trivial. These make it a good example function to study backprop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "82be75bb-2239-4936-a9bd-ed119591e1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5701, -0.0628, -1.8929, -0.9927, -1.4878], requires_grad=True)\n",
      "tensor([0.1098, 0.4958, 0.0795, 0.1956, 0.1192], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([-0.0072, -0.1155,  0.0580,  0.1067, -0.0420])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "x = torch.randn((5,), requires_grad=True) # a test input vector\n",
    "y = softmax(x, dim=-1)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "y_grad = torch.randn((5,), requires_grad=False) \n",
    "\n",
    "y.backward(y_grad, retain_graph=True)\n",
    "print(x.grad)\n",
    "#assert torch.allclose(x.grad, y.grad_fn(y_grad)) # same as calling y.grad_fn()\n",
    "#assert torch.allclose(x.grad, torch.autograd.grad(y, x, y_grad)[0]) # an alternative way to get x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce44cfb-ff4c-4a11-8f07-5176757a5a24",
   "metadata": {},
   "source": [
    "Assume the hypotheticcal scaler loss function (at the root of our computation graph) is $L$, the output value `x.grad` $\\nabla_x^T L $, according to the [chain rule](../math/chain_rule_and_jacobian.ipynb), is derived from $\\nabla_{y}^T L \\cdot J_x y$ where  $\\nabla_{y} L$ is just `y_grad`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d2c75-f7b6-4364-9820-10ee498bd5b5",
   "metadata": {},
   "source": [
    "## Softmax (Naively Batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff46c96-c0e6-442d-a721-49f8c903355d",
   "metadata": {},
   "source": [
    "We can also compute the \"batched\" version. In backprop context, batch means the same and (non-batched) input vector $x$ with (batched) `y_grad` (if $x$ are batched and are different, the Jacobian matrix's values $\\left. J_{x}y \\right|_{x = x_1, x_2, ..., x_B}$ would be different):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "84793e04-929b-4e7f-8805-c90810b0aa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1278, -0.2947,  0.6674,  0.4836, -0.4141],\n",
      "        [-0.9572,  0.3029,  1.2252,  0.3351,  0.7663],\n",
      "        [-0.9017,  2.3485, -0.6726,  0.6428,  0.2113]])\n",
      "tensor([-0.0072, -0.1155,  0.0580,  0.1067, -0.0420])\n",
      "tensor([-0.1380,  0.0017,  0.0736,  0.0070,  0.0557])\n",
      "tensor([-0.2267,  0.5879, -0.1459, -0.1017, -0.1134])\n"
     ]
    }
   ],
   "source": [
    "Y_grad = torch.randn((3, 5), requires_grad=False)  # a test **batch**\n",
    "# let's set its first row to be identical to our above vector, just make it easier to observe the differences.\n",
    "Y_grad[0, :] = y_grad.detach()\n",
    "print(Y_grad)\n",
    "\n",
    "# y = softmax(x, dim=-1) # same x, same, y, no need to do it again!\n",
    "\n",
    "x.grad.zero_()\n",
    "y.backward(Y_grad[0, :], retain_graph=True)\n",
    "print(x.grad) # same as the old x.grad\n",
    "\n",
    "x.grad.zero_()\n",
    "y.backward(Y_grad[1, :], retain_graph=True)\n",
    "print(x.grad)\n",
    "\n",
    "x.grad.zero_()\n",
    "y.backward(Y_grad[2, :], retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7201c6ed-442e-4e83-8110-f2ff313fb8c3",
   "metadata": {},
   "source": [
    "## VJP- and JVP-Batched\n",
    "A VJP (Vectorâ€“Jacobian Product) is simply a term to describe the math form:\n",
    "$$\n",
    "\\underbrace{\\mathbf{v}^\\top}_{\\text{cotangents}}\n",
    "\\underbrace{\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathbf{x}}}_{\\substack{\\text{Jacobian of }f \\\\ \\text{w.r.t. primals}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfa14ef-2f4f-45a7-bb12-a6e5c95967a9",
   "metadata": {},
   "source": [
    "similarly, JVP is\n",
    "$$\n",
    "\\underbrace{\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathbf{x}}}_{\\substack{\\text{Jacobian of }f \\\\ \\text{w.r.t. primals}}}\n",
    "\\underbrace{\\mathbf{u}}_{\\text{tangents}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "15e686ae-3642-4235-923e-dfac907563df",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2, vjp_fn = torch.func.vjp(softmax, x) # torch.func.vjp(func, *primals)\n",
    "assert torch.allclose(y, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "298ab353-764e-4c19-a1d3-dc8f2e6fdde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0072, -0.1155,  0.0580,  0.1067, -0.0420],\n",
       "        grad_fn=<SoftmaxBackwardDataBackward0>),)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vjp_fn(y_grad) # y_grad is the \"v\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cc7205-858f-4f85-9b9f-a387d20cdae0",
   "metadata": {},
   "source": [
    "However, `vjp_fn` cannot take in batched y_grad, i.e., Y_grad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f38fd8a5-3c15-47f6-9de1-cef720099a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch in shape: grad_output[0] has a shape of torch.Size([3, 5]) and output[0] has a shape of torch.Size([5]).\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vjp_fn(Y_grad)  # error!\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62723ee3-c863-47be-ad31-ff604cf7ef1c",
   "metadata": {},
   "source": [
    "A simple solution is to loop and apply (i.e., `vmap`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d87aeb03-b8fc-4776-9ec6-bc5e6a391ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0072, -0.1155,  0.0580,  0.1067, -0.0420],\n",
       "        [-0.1380,  0.0017,  0.0736,  0.0070,  0.0557],\n",
       "        [-0.2267,  0.5879, -0.1459, -0.1017, -0.1134]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_vjp_fn = torch.func.vmap(vjp_fn)\n",
    "vmap_X_grad = batched_vjp_fn(Y_grad)[0].detach()\n",
    "assert torch.allclose(vmap_X_grad, torch.vstack([\n",
    "    vjp_fn(Y_grad[0, :])[0].detach(),\n",
    "    vjp_fn(Y_grad[1, :])[0].detach(),\n",
    "    vjp_fn(Y_grad[2, :])[0].detach(),\n",
    "]))\n",
    "vmap_X_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c80bdf-3817-4d35-b173-2703472fc07d",
   "metadata": {},
   "source": [
    "where each row corresponds to exactly the single-batched x.grad after each `y.backward` above.\n",
    "\n",
    "Due to the efficiency of only compute Jacobian matrix once (via the `vjp` function), this batched version runs faster.$^{1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1324caa4-d397-4fdc-87b2-0af05ec61838",
   "metadata": {},
   "source": [
    "Similarly, for `jvp` (note that pytorch jvp takes in tuples of *multiple primals or tangents* compared to `vjp`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ae1d93de-9c88-4b71-9a62-ad6202b3430f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3941, -1.3663,  0.7759,  0.4256,  0.6555])\n",
      "tensor([ 0.0284, -0.1503,  0.0211,  0.0654,  0.0355], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# reset\n",
    "x.grad.zero_()\n",
    "y = softmax(x, dim=-1)\n",
    "y_grad = torch.randn((5,), requires_grad=False)\n",
    "y.backward(y_grad)\n",
    "print(y_grad)\n",
    "\n",
    "# Move vjp here for easy comparison:\n",
    "# y2, vjp_fn = torch.func.vjp(softmax, x) # torch.func.vjp(func, *primals)\n",
    "y3, jvp_out = torch.func.jvp(softmax, (x, ), (x.grad, )) # torch.func.jvp(func, primals, tangents)\n",
    "assert torch.allclose(y, y3)\n",
    "print(jvp_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd065fca-ff19-496e-a67d-bdffa38b11d3",
   "metadata": {},
   "source": [
    "## Jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63501fd-3b83-4abe-8e22-79eb40724a59",
   "metadata": {},
   "source": [
    "Above manipulations all hide the actual Jacobian matrix from our sight. How to retrieve it? Simple! Just constract an eye matrix and retrieve it row by row:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380cd766-856c-4186-98c3-deae720b8e14",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "e_1 \\\\\n",
    "e_2 \\\\\n",
    "... \\\\\n",
    "e_5\n",
    "\\end{bmatrix}\n",
    "\\cdot [ J_x \\; y(x) ]_{5 \\times 5} = E_{5 \\times 5} \\cdot [ J_x \\; y(x) ]_{5 \\times 5} = [ J_x \\; y(x) ]_{5 \\times 5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8d7d156c-6508-4333-9b50-bcfc81b24fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0978, -0.0544, -0.0087, -0.0215, -0.0131],\n",
      "        [-0.0544,  0.2500, -0.0394, -0.0970, -0.0591],\n",
      "        [-0.0087, -0.0394,  0.0732, -0.0156, -0.0095],\n",
      "        [-0.0215, -0.0970, -0.0156,  0.1574, -0.0233],\n",
      "        [-0.0131, -0.0591, -0.0095, -0.0233,  0.1050]])\n"
     ]
    }
   ],
   "source": [
    "eye = torch.eye(5, 5)\n",
    "J = batched_vjp_fn(eye)[0].detach()\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbd8f9a-6683-4d11-999f-b7ea765cbe55",
   "metadata": {},
   "source": [
    "Let's verify whether this matrix is indeed the Jacobian matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d0322963-c8af-4cb6-8401-984d289f3d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad.zero_()\n",
    "y = softmax(x, dim=-1)\n",
    "y_grad = torch.randn((5,), requires_grad=False)\n",
    "y.backward(y_grad)\n",
    "assert torch.allclose(x.grad, y_grad @ J)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1399b53-5a38-4c06-bc90-2d423f3c98ec",
   "metadata": {},
   "source": [
    "The above vjp-vmap composition to get the Jacobian matrix can be also replaced by the `jacrev` function, which stands for \"Jacobian-of-Reverse\" in the reverse mode autodiff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4e61d367-c29d-450d-8992-3bf17cd1df51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0978, -0.0544, -0.0087, -0.0215, -0.0131],\n",
       "        [-0.0544,  0.2500, -0.0394, -0.0970, -0.0591],\n",
       "        [-0.0087, -0.0394,  0.0732, -0.0156, -0.0095],\n",
       "        [-0.0215, -0.0970, -0.0156,  0.1574, -0.0233],\n",
       "        [-0.0131, -0.0591, -0.0095, -0.0233,  0.1050]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.func.jacrev(softmax)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a2187-ba58-4aa2-b4f6-82ea1297a367",
   "metadata": {},
   "source": [
    "Similarly, there is a `jacfwd` for forward mode autodiff.\n",
    "`jacfwd` uses forward-mode AD. It is implemented as a composition of jvp-vmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e61a32a9-5366-408c-b55a-c3c85dbf2507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0978, -0.0544, -0.0087, -0.0215, -0.0131],\n",
       "        [-0.0544,  0.2500, -0.0394, -0.0970, -0.0591],\n",
       "        [-0.0087, -0.0394,  0.0732, -0.0156, -0.0095],\n",
       "        [-0.0215, -0.0970, -0.0156,  0.1574, -0.0233],\n",
       "        [-0.0131, -0.0591, -0.0095, -0.0233,  0.1050]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.func.jacfwd(softmax)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dbba9c-58b6-4c7d-9439-5d61f15820a1",
   "metadata": {},
   "source": [
    "`jacfwd` and `jacrev` can be substituted for each other but they have different performance characteristics:$^1$\n",
    "> In reverse-mode AD, we are computing the jacobian row-by-row, while in forward-mode AD (which computes Jacobian-vector products), we are computing it column-by-column. The Jacobian matrix has M rows and N columns, so if it is taller or wider one way we may prefer the method that deals with fewer rows or columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c9823-e925-4083-a88b-f1f8ae2a82e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3131e5c6-06a9-476a-8188-23b09411ee29",
   "metadata": {},
   "source": [
    "## Reference\n",
    "1. https://docs.pytorch.org/functorch/stable/notebooks/jacobians_hessians.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af4472-4918-4a5f-8b49-6c79a3a799de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
