{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa13f3a3-574e-4cd5-956a-28c33e5aa5d4",
   "metadata": {},
   "source": [
    "# Pytorch Backprop (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea98b97-f3f2-4c4a-a37c-ca3131e6546e",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "Let's first focus on the softmax function which is a $x \\to y: R^{n} \\rightarrow R^{n}$ mapping where any output element depends on all input elements, and its Jacobian matrix is somewhat non-trivial. These make it a good example function to study backprop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "82be75bb-2239-4936-a9bd-ed119591e1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.8477,  0.8658,  1.0925,  1.8025, -0.5439], requires_grad=True)\n",
      "tensor([0.3458, 0.1295, 0.1625, 0.3305, 0.0316], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([ 0.0801,  0.0278,  0.2022, -0.3059, -0.0042])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "x = torch.randn((5,), requires_grad=True) # a test input vector\n",
    "y = softmax(x, dim=-1)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "y_grad = torch.randn((5,), requires_grad=False) \n",
    "\n",
    "y.backward(y_grad, retain_graph=True)\n",
    "print(x.grad)\n",
    "#assert torch.allclose(x.grad, y.grad_fn(y_grad)) # same as calling y.grad_fn()\n",
    "#assert torch.allclose(x.grad, torch.autograd.grad(y, x, y_grad)[0]) # an alternative way to get x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce44cfb-ff4c-4a11-8f07-5176757a5a24",
   "metadata": {},
   "source": [
    "Assume the hypotheticcal scaler loss function (at the root of our computation graph) is $L$, the output value `x.grad` $\\nabla_x^T L $, according to the [chain rule](../math/chain_rule_and_jacobian.ipynb), is derived from $\\nabla_{y}^T L \\cdot J_x y$ where  $\\nabla_{y} L$ is just `y_grad`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d2c75-f7b6-4364-9820-10ee498bd5b5",
   "metadata": {},
   "source": [
    "## Softmax (Batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff46c96-c0e6-442d-a721-49f8c903355d",
   "metadata": {},
   "source": [
    "We can also compute the \"batched\" version. In backprop context, batch means the same and (non-batched) input vector $x$ with (batched) `y_grad` (if $x$ are batched and are different, the Jacobian matrix's values $\\left. J_{x}y \\right|_{x = x_1, x_2, ..., x_B}$ would be different, thus it gives worse utilization of the hardware):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84793e04-929b-4e7f-8805-c90810b0aa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0335, -0.0504,  0.9793, -1.1906, -0.3986],\n",
      "        [ 0.1053,  0.5013, -0.1292, -1.2227,  1.3505],\n",
      "        [-0.2068, -1.4909,  0.5869,  1.4332, -1.1067]])\n",
      "tensor([ 0.0801,  0.0278,  0.2022, -0.3059, -0.0042])\n",
      "tensor([ 0.1336,  0.1013,  0.0247, -0.3112,  0.0516])\n",
      "tensor([-0.1647, -0.2280,  0.0516,  0.3846, -0.0435])\n"
     ]
    }
   ],
   "source": [
    "Y_grad = torch.randn((3, 5), requires_grad=False)  # a test **batch**\n",
    "# let's set its first row to be identical to our above vector, just make it easier to observe the differences.\n",
    "Y_grad[0, :] = y_grad.detach()\n",
    "print(Y_grad)\n",
    "\n",
    "# y = softmax(x, dim=-1) # same x, same, y, no need to do it again!\n",
    "\n",
    "x.grad.zero_()\n",
    "y.backward(Y_grad[0, :], retain_graph=True)\n",
    "print(x.grad) # same as the old x.grad\n",
    "\n",
    "x.grad.zero_()\n",
    "y.backward(Y_grad[1, :], retain_graph=True)\n",
    "print(x.grad)\n",
    "\n",
    "x.grad.zero_()\n",
    "y.backward(Y_grad[2, :], retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7201c6ed-442e-4e83-8110-f2ff313fb8c3",
   "metadata": {},
   "source": [
    "## VJP and JVP\n",
    "A VJP (Vectorâ€“Jacobian Product) is simply a term to describe the math form:\n",
    "$$\n",
    "\\underbrace{\\mathbf{v}^\\top}_{\\text{cotangents}}\n",
    "\\underbrace{\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathbf{x}}}_{\\substack{\\text{Jacobian of }f \\\\ \\text{w.r.t. primals}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfa14ef-2f4f-45a7-bb12-a6e5c95967a9",
   "metadata": {},
   "source": [
    "similarly, JVP is\n",
    "$$\n",
    "\\underbrace{\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathbf{x}}}_{\\substack{\\text{Jacobian of }f \\\\ \\text{w.r.t. primals}}}\n",
    "\\underbrace{\\mathbf{u}}_{\\text{tangents}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "15e686ae-3642-4235-923e-dfac907563df",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2, vjp_fn = torch.func.vjp(softmax, x) # torch.func.vjp(func, *primals)\n",
    "assert torch.allclose(y, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "298ab353-764e-4c19-a1d3-dc8f2e6fdde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0801,  0.0278,  0.2022, -0.3059, -0.0042],\n",
       "        grad_fn=<SoftmaxBackwardDataBackward0>),)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vjp_fn(y_grad) # y_grad is the \"v\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cc7205-858f-4f85-9b9f-a387d20cdae0",
   "metadata": {},
   "source": [
    "However, `vjp_fn` cannot take in batched y_grad, i.e., Y_grad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f38fd8a5-3c15-47f6-9de1-cef720099a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch in shape: grad_output[0] has a shape of torch.Size([3, 5]) and output[0] has a shape of torch.Size([5]).\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vjp_fn(Y_grad)  # error!\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62723ee3-c863-47be-ad31-ff604cf7ef1c",
   "metadata": {},
   "source": [
    "A simple solution is to loop and apply (i.e., `vmap`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d87aeb03-b8fc-4776-9ec6-bc5e6a391ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0801,  0.0278,  0.2022, -0.3059, -0.0042],\n",
       "        [ 0.1336,  0.1013,  0.0247, -0.3112,  0.0516],\n",
       "        [-0.1647, -0.2280,  0.0516,  0.3846, -0.0435]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_vjp_fn = torch.func.vmap(vjp_fn)\n",
    "vmap_X_grad = batched_vjp_fn(Y_grad)[0].detach()\n",
    "assert torch.allclose(vmap_X_grad, torch.vstack([\n",
    "    vjp_fn(Y_grad[0, :])[0].detach(),\n",
    "    vjp_fn(Y_grad[1, :])[0].detach(),\n",
    "    vjp_fn(Y_grad[2, :])[0].detach(),\n",
    "]))\n",
    "vmap_X_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c80bdf-3817-4d35-b173-2703472fc07d",
   "metadata": {},
   "source": [
    "where each row corresponds to exactly the single-batched x.grad after each `y.backward` above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1399b53-5a38-4c06-bc90-2d423f3c98ec",
   "metadata": {},
   "source": [
    "Note that this vmap-vjp composition can be conveniently wrapped in to the `jacrev` function, which stands for \"Jacobian-Reverse\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e61d367-c29d-450d-8992-3bf17cd1df51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2262, -0.0448, -0.0562, -0.1143, -0.0109],\n",
       "        [-0.0448,  0.1128, -0.0210, -0.0428, -0.0041],\n",
       "        [-0.0562, -0.0210,  0.1361, -0.0537, -0.0051],\n",
       "        [-0.1143, -0.0428, -0.0537,  0.2213, -0.0105],\n",
       "        [-0.0109, -0.0041, -0.0051, -0.0105,  0.0306]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.func.jacrev(softmax)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1d81e4-c7a4-4bba-ac3a-343296b53fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
