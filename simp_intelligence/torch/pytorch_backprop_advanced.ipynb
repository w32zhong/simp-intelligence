{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa13f3a3-574e-4cd5-956a-28c33e5aa5d4",
   "metadata": {},
   "source": [
    "# Pytorch Backprop (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea98b97-f3f2-4c4a-a37c-ca3131e6546e",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "Let's first focus on the softmax function which is a $x \\to y: R^{n} \\rightarrow R^{n}$ mapping where any output element depends on all input elements, and its Jacobian matrix is somewhat non-trivial. These make it a good example function to study backprop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "82be75bb-2239-4936-a9bd-ed119591e1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0040, -1.5440,  1.4854,  0.8328,  0.3945], requires_grad=True)\n",
      "tensor([0.2449, 0.0192, 0.3964, 0.2064, 0.1331], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([-0.2268, -0.0136,  0.1423, -0.1827,  0.2808])\n",
      "(tensor([-0.2268, -0.0136,  0.1423, -0.1827,  0.2808]),)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "x = torch.randn((5,), requires_grad=True) # a test input vector\n",
    "y = softmax(x, dim=-1)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "y_grad = torch.randn((5,), requires_grad=True) \n",
    "\n",
    "y.backward(y_grad, retain_graph=True)\n",
    "assert torch.allclose(x.grad, y.grad_fn(y_grad))\n",
    "print(x.grad)\n",
    "print(torch.autograd.grad(y, x, y_grad)) # an alternative way to get v.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce44cfb-ff4c-4a11-8f07-5176757a5a24",
   "metadata": {},
   "source": [
    "Assume the hypotheticcal scaler loss function (at the root of our computation graph) is $L$, the output value `x.grad` $\\nabla_x^T L $, according to the [chain rule](../math/chain_rule_and_jacobian.ipynb), is derived from $\\nabla_{y}^T L \\cdot J_x y$ where  $\\nabla_{y} L$ is just `y_grad`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d2c75-f7b6-4364-9820-10ee498bd5b5",
   "metadata": {},
   "source": [
    "## Softmax (Batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff46c96-c0e6-442d-a721-49f8c903355d",
   "metadata": {},
   "source": [
    "We can also compute the \"batched\" version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "84793e04-929b-4e7f-8805-c90810b0aa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0040, -1.5440,  1.4854,  0.8328,  0.3945],\n",
      "        [ 1.2526,  1.3910, -0.9862, -0.6285,  0.0700],\n",
      "        [ 0.3329,  0.6859,  1.6928, -1.2989,  0.0233]], requires_grad=True)\n",
      "tensor([[0.2449, 0.0192, 0.3964, 0.2064, 0.1331],\n",
      "        [0.3685, 0.4232, 0.0393, 0.0562, 0.1129],\n",
      "        [0.1380, 0.1964, 0.5375, 0.0270, 0.1012]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-0.2268, -0.0136,  0.1423, -0.1827,  0.2808],\n",
      "        [-0.1579,  0.3427, -0.0167, -0.0944, -0.0736],\n",
      "        [-0.0404,  0.3583, -0.3998,  0.0679,  0.0140]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn((3, 5), requires_grad=False) # a test **matrix**\n",
    "Y_grad = torch.randn((3, 5), requires_grad=False) \n",
    "\n",
    "# let's set its first row to be identical to our above vector, just make it easier to observe the differences.\n",
    "X[0, :] = x.detach() \n",
    "Y_grad[0, :] = y_grad.detach()\n",
    "\n",
    "X.requires_grad = True\n",
    "Y_grad.requires_grad = True\n",
    "\n",
    "# now perform the batched version:\n",
    "Y = softmax(X, dim=-1)\n",
    "print(X)\n",
    "print(Y)\n",
    "Y.backward(Y_grad)\n",
    "print(X.grad)\n",
    "\n",
    "assert torch.allclose(X.grad[0, :], x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7201c6ed-442e-4e83-8110-f2ff313fb8c3",
   "metadata": {},
   "source": [
    "## VJP and JVP\n",
    "A VJP (Vectorâ€“Jacobian Product) is simply a term to describe the math form:\n",
    "$$\n",
    "\\underbrace{\\mathbf{v}^\\top}_{\\text{cotangents}}\n",
    "\\underbrace{\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathbf{x}}}_{\\substack{\\text{Jacobian of }f \\\\ \\text{w.r.t. primals}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfa14ef-2f4f-45a7-bb12-a6e5c95967a9",
   "metadata": {},
   "source": [
    "similarly, JVP is\n",
    "$$\n",
    "\\underbrace{\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathbf{x}}}_{\\substack{\\text{Jacobian of }f \\\\ \\text{w.r.t. primals}}}\n",
    "\\underbrace{\\mathbf{u}}_{\\text{tangents}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "15e686ae-3642-4235-923e-dfac907563df",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2, vjp_fn = torch.func.vjp(softmax, x) # torch.func.vjp(func, *primals)\n",
    "assert torch.allclose(y, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "298ab353-764e-4c19-a1d3-dc8f2e6fdde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.2268, -0.0136,  0.1423, -0.1827,  0.2808],\n",
       "        grad_fn=<SoftmaxBackwardDataBackward0>),)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vjp_fn(y_grad) # y_grad is the \"v\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cc7205-858f-4f85-9b9f-a387d20cdae0",
   "metadata": {},
   "source": [
    "However, `vjp_fn` cannot take in batched y_grad, i.e., Y_grad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f38fd8a5-3c15-47f6-9de1-cef720099a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch in shape: grad_output[0] has a shape of torch.Size([3, 5]) and output[0] has a shape of torch.Size([5]).\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vjp_fn(Y_grad)  # error!\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62723ee3-c863-47be-ad31-ff604cf7ef1c",
   "metadata": {},
   "source": [
    "A simple solution is to loop and apply (i.e., `vmap`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d87aeb03-b8fc-4776-9ec6-bc5e6a391ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2268, -0.0136,  0.1423, -0.1827,  0.2808],\n",
       "        [ 0.0644,  0.0288,  0.1057, -0.2041,  0.0053],\n",
       "        [-0.1223,  0.0310, -0.3767,  0.4771, -0.0091]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_vjp_fn = torch.func.vmap(vjp_fn)\n",
    "vmap_X_grad = batched_vjp_fn(Y_grad)[0].detach()\n",
    "vmap_X_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "ed1b7c4c-684b-431f-9c5b-cd7d4d4e6011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True]])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vmap_X_grad == torch.vstack([\n",
    "    vjp_fn(Y_grad[0, :])[0].detach(),\n",
    "    vjp_fn(Y_grad[1, :])[0].detach(),\n",
    "    vjp_fn(Y_grad[2, :])[0].detach(),\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
