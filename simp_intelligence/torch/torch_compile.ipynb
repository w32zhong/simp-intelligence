{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20c11a7e-4125-4327-96b6-3fff1584d6a3",
   "metadata": {},
   "source": [
    "`torch.compile` is introduced in PyTorch 2.0 and is intended to replace TorchScript (`torch.jit`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d6a05-88ee-4cb1-ba1d-8e59a76b8952",
   "metadata": {},
   "source": [
    "## Overall\n",
    "A torch compiled Python code will go through two stages: TorchDynamo + Inductor:\n",
    "\n",
    "1. TorchDynamo\n",
    "Parse Python code and get Python byte code, then generate a \"FX Graph\".\n",
    "\n",
    "2. TorchInductor\n",
    "Convert FX Graph into efficient code with potential Operator Fusion.\n",
    "\n",
    "The higher-level FX Graph operators (e.g., aten.add) will be converted into a loop-level IR in which operation fusion will be performed and are merged into one loop (e.g., Add + ReLU).\n",
    "\n",
    "Then, it decides how to introduce hardware-dependent code into these loops (e.g., adding Tile size and `tl.program_id`).\n",
    "\n",
    "Finally, in codegen, these loops are processed with a Triton template engine to generate Python `@triton.jit` code (saved at `/tmp/torchinductor_xxx`).\n",
    "TorchInductor utilize Triton as a backend to further optimize the generated code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf860251-a54c-40a9-8498-50aaf2eeda25",
   "metadata": {},
   "source": [
    "## Example Dissect\n",
    "Create a toy example `torch_compile_builtin_fusion.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81365ba1-2894-43c4-9d68-4f7217c6c853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0060, 0.0038, 0.0091,  ..., 0.0089, 0.0004, 0.0031],\n",
      "        [0.0034, 0.0020, 0.0045,  ..., 0.0048, 0.0013, 0.0002],\n",
      "        [0.0031, 0.0067, 0.0035,  ..., 0.0387, 0.0001, 0.0006],\n",
      "        ...,\n",
      "        [0.0035, 0.0028, 0.0057,  ..., 0.0206, 0.0006, 0.0003],\n",
      "        [0.0050, 0.0026, 0.0029,  ..., 0.0086, 0.0005, 0.0034],\n",
      "        [0.0013, 0.0067, 0.0165,  ..., 0.0294, 0.0002, 0.0005]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand((100, 100), device='cuda')\n",
    "b = torch.rand((100, 100), device='cuda')\n",
    "\n",
    "def fn(x, y):\n",
    "    z = torch.matmul(x, y)\n",
    "    return torch.nn.functional.softmax(z, dim=1)\n",
    "\n",
    "compiled_fn = torch.compile(fn)\n",
    "print(compiled_fn(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc39e8f7-c90c-42f9-9163-716463c2bc39",
   "metadata": {},
   "source": [
    "We can print the two-stage outcome: `graph_code` (FX Graph code presentation), and `output_code` are the output Triton code by Inductor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7e0f32d-3765-41d4-a9c8-b020988d4c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V0113 15:50:31.765000 550912 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code] TRACED GRAPH\n",
      "V0113 15:50:31.765000 550912 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]  ===== __compiled_fn_1_ce4531cc_6d02_43c8_8060_d25081027612 =====\n",
      "V0113 15:50:31.765000 550912 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]  /home/tk/Desktop/jupyter/simp-intelligence/.pixi/envs/default/lib/python3.13/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0113 15:50:31.765000 550912 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]     def forward(self, L_y_: \"\u001b[31mf32\u001b[0m\u001b[34m[100, 100]\u001b[0m\u001b[2m\u001b[34m[100, 1]\u001b[0m\u001b[2m\u001b[32mcuda:0\u001b[0m\", L_x_: \"\u001b[31mf32\u001b[0m\u001b[34m[100, 100]\u001b[0m\u001b[2m\u001b[34m[100, 1]\u001b[0m\u001b[2m\u001b[32mcuda:0\u001b[0m\"):\n",
      "V0113 15:50:31.765000 550912 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         l_y_ = L_y_\n",
      "V0113 15:50:31.765000 550912 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         l_x_ = L_x_\n",
      "V0113 15:50:31.765000 550912 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         \n",
      "V0113 15:50:31.765000 550912 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]          \u001b[2m# File: /home/tk/Desktop/jupyter/simp-intelligence/simp_intelligence/torch/torch_compile_builtin_fusion.py:7 in fn, code: z = torch.matmul(x, y)\u001b[0m\n",
      "V0113 15:50:31.765000 550912 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         z: \"\u001b[31mf32\u001b[0m\u001b[34m[100, 100]\u001b[0m\u001b[2m\u001b[34m[100, 1]\u001b[0m\u001b[2m\u001b[32mcuda:0\u001b[0m\" = torch.matmul(l_x_, l_y_);  \u001b[2ml_x_ = l_y_ = None\u001b[0m\n",
      "V0113 15:50:31.765000 550912 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         \n",
      "V0113 15:50:31.765000 550912 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]          \u001b[2m# File: /home/tk/Desktop/jupyter/simp-intelligence/simp_intelligence/torch/torch_compile_builtin_fusion.py:8 in fn, code: return torch.nn.functional.softmax(z, dim=1)\u001b[0m\n",
      "V0113 15:50:31.765000 550912 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         softmax: \"\u001b[31mf32\u001b[0m\u001b[34m[100, 100]\u001b[0m\u001b[2m\u001b[34m[100, 1]\u001b[0m\u001b[2m\u001b[32mcuda:0\u001b[0m\" = torch.nn.functional.softmax(z, dim = \u001b[34m1\u001b[0m);  \u001b[2mz = None\u001b[0m\n",
      "V0113 15:50:31.765000 550912 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         return (softmax,)\n",
      "V0113 15:50:31.765000 550912 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         \n",
      "V0113 15:50:31.765000 550912 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code] \n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] Output code: \n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # AOT ID: ['0_inference']\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import torch\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import math\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import random\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import os\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import tempfile\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from math import inf, nan\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from cmath import nanj\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.utils import maybe_profile\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch import device, empty_strided\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import triton\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import triton.language as tl\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] aten = torch.ops.aten\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] inductor_ops = torch.ops.inductor\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] _quantized = torch.ops._quantized\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] async_compile = AsyncCompile()\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tk/ak/cakvrozgrag4reyjvpd7hik2cnclzuht4g7fcumnp2l43selmyqm.py\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # Topologically Sorted Source Nodes: [softmax], Original ATen: [aten._softmax]\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # Source node to ATen node mapping:\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] #   softmax => div\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # Graph fragment:\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] #   %prepare_softmax_online_default : [num_users=2] = call_function[target=torch.ops.prims.prepare_softmax_online.default](args = (%mm, 1), kwargs = {})\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] #   %sub_tensor : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%mm, %getitem), kwargs = {})\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] #   %exp_default : [num_users=1] = call_function[target=torch.ops.aten.exp.default](args = (%sub_tensor,), kwargs = {})\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] #   %div : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%exp_default, %getitem_1), kwargs = {})\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] triton_per_fused__softmax_0 = async_compile.triton('triton_per_fused__softmax_0', '''\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import triton\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import triton.language as tl\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] @triton_heuristics.persistent_reduction(\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     size_hints={'x': 128, 'r0_': 128},\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     reduction_hint=ReductionHint.INNER,\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     filename=__file__,\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     triton_meta={'signature': {'in_out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=28, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]]}]},\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__softmax_0', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 4, 'backend_hash': '7D055AB65A5FF60E1050B050060B47798E877F6A1CE347B72B5DF848DBA8494C', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 0, 'r0_': 120000}}\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] )\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] @triton.jit\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] def triton_per_fused__softmax_0(in_out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr):\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     xnumel = 100\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     r0_numel = 100\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     R0_BLOCK: tl.constexpr = 128\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     rnumel = r0_numel\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     RBLOCK: tl.constexpr = R0_BLOCK\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     xmask = xindex < xnumel\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     r0_index = tl.arange(0, R0_BLOCK)[None, :]\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     r0_offset = 0\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     r0_mask = r0_index < r0_numel\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     roffset = r0_offset\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     rindex = r0_index\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     r0_1 = r0_index\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     x0 = xindex\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp0 = tl.load(in_out_ptr0 + (r0_1 + 100*x0), r0_mask & xmask, other=0.0)\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp3 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp5 = tl.where(r0_mask & xmask, tmp3, float(\"-inf\"))\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp6 = triton_helpers.max2(tmp5, 1)[:, None]\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp7 = tmp1 - tmp6\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp8 = tl_math.exp(tmp7)\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp9 = tl.broadcast_to(tmp8, [XBLOCK, R0_BLOCK])\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp11 = tl.where(r0_mask & xmask, tmp9, 0)\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp12 = tl.sum(tmp11, 1)[:, None]\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp13 = tmp0 - tmp6\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp14 = tl_math.exp(tmp13)\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp15 = (tmp14 / tmp12)\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tl.store(in_out_ptr0 + (r0_1 + 100*x0), tmp15, r0_mask & xmask)\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] ''', device_str='cuda')\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] async_compile.wait(globals())\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] del async_compile\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] def call(args):\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     arg0_1, arg1_1 = args\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     args.clear()\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     assert_size_stride(arg0_1, (100, 100), (100, 1))\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     assert_size_stride(arg1_1, (100, 100), (100, 1))\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     with torch.cuda._DeviceGuard(0):\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         torch.cuda.set_device(0)\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         buf0 = empty_strided_cuda((100, 100), (100, 1), torch.float32)\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         # Topologically Sorted Source Nodes: [z], Original ATen: [aten.mm]\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         extern_kernels.mm(arg1_1, arg0_1, out=buf0)\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         del arg0_1\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         del arg1_1\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         buf3 = buf0; del buf0  # reuse\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         # Topologically Sorted Source Nodes: [softmax], Original ATen: [aten._softmax]\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         stream0 = get_raw_stream(0)\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         triton_per_fused__softmax_0.run(buf3, 100, 100, stream=stream0)\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     return (buf3, )\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     from torch._inductor.utils import print_performance\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     arg0_1 = rand_strided((100, 100), (100, 1), device='cuda:0', dtype=torch.float32)\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     arg1_1 = rand_strided((100, 100), (100, 1), device='cuda:0', dtype=torch.float32)\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     fn = lambda: call([arg0_1, arg1_1])\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] if __name__ == \"__main__\":\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)\n",
      "V0113 15:50:32.940000 550912 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:50:32.942000 550912 site-packages/torch/_inductor/codecache.py:1237] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tk/4s/c4sezo6ijxma66v5wddsot6hferhctvtsdh3sv6ruk5urg3qz7iu.py\n",
      "tensor([[0.0083, 0.0034, 0.0010,  ..., 0.0067, 0.0032, 0.0017],\n",
      "        [0.0058, 0.0025, 0.0005,  ..., 0.0013, 0.0053, 0.0028],\n",
      "        [0.0043, 0.0058, 0.0030,  ..., 0.0018, 0.0063, 0.0038],\n",
      "        ...,\n",
      "        [0.0069, 0.0038, 0.0034,  ..., 0.0022, 0.0097, 0.0019],\n",
      "        [0.0207, 0.0057, 0.0091,  ..., 0.0003, 0.0078, 0.0010],\n",
      "        [0.0065, 0.0087, 0.0021,  ..., 0.0021, 0.0076, 0.0020]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "!TORCH_LOGS=\"graph_code,output_code\" python torch_compile_builtin_fusion.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d075b1-025f-4982-806e-299cbf21f430",
   "metadata": {},
   "source": [
    "As shown above, the generated Triton code is using a fused function `triton_per_fused__softmax_0`.\n",
    "\n",
    "We can also pass in more specific flags to output the intermediate steps before and after fusion taking palce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9da6a01-b6e5-49a8-8d40-bda942ee0a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tk/Desktop/jupyter/simp-intelligence/.pixi/envs/default/lib/python3.13/site-packages/torch/_dynamo/pgo.py:525: UserWarning: dynamo_pgo force disabled by torch._inductor.config.force_disable_caches\n",
      "  warn_once(\n",
      "/home/tk/Desktop/jupyter/simp-intelligence/.pixi/envs/default/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:282: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "W0113 15:50:38.425000 550945 site-packages/torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] BEFORE FUSION\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op0: ExternKernelSchedulerNode(ExternKernelOut)\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op0.writes = [StarDep(name='buf0', mode=None)]\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op0.unmet_dependencies = []\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op0.met_dependencies = [StarDep(name='arg0_1', mode=None), StarDep(name='arg1_1', mode=None)]\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op0.outputs = [\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     buf0: ExternKernelOut\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     buf0.layout = FixedLayout('cuda:0', torch.float32, size=[100, 100], stride=[100, 1])\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     buf0.users = [\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         NodeUser(node=SchedulerNode(name='op1'), can_inplace=False, is_weak=False),\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         NodeUser(node=SchedulerNode(name='op2'), can_inplace=False, is_weak=False),\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         NodeUser(node=SchedulerNode(name='op3'), can_inplace=True, is_weak=False),\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     ]\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] ]\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op0.node.kernel = extern_kernels.mm\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] \n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] \n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op1: SchedulerNode(ComputedBuffer)\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op1.writes = [MemoryDep('buf1', c0, {c0: 100})]\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op1.unmet_dependencies = [MemoryDep('buf0', c0, {c0: 10000})]\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op1.met_dependencies = []\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op1.outputs = [\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     buf1: ComputedBuffer\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     buf1.layout = FixedLayout('cuda:0', torch.float32, size=[100, 1], stride=[1, 100])\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     buf1.users = [NodeUser(node=SchedulerNode(name='op3'), can_inplace=False, is_weak=False)]\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] ]\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op1.group.device = cuda:0\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op1.group.iteration = (100, 100)\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op1.sizes = ([100], [100])\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] buf0_layout = FixedLayout('cuda:0', torch.float32, size=[100, 100], stride=[100, 1])\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] buf1_layout = FixedLayout('cuda:0', torch.float32, size=[100, 1], stride=[1, 100])\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] class op1_loop_body:\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     var_ranges = {p0: 100, p1: 100}\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     index0 = 100*p0 + p1\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     index1 = p0\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     def body(self, ops):\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         get_index = self.get_index('index0')\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         load = ops.load('buf0', get_index)\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         reduction = ops.reduction(torch.float32, torch.float32, 'online_softmax_reduce', load)\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         getitem = reduction[0]\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         getitem_1 = reduction[1]\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         get_index_1 = self.get_index('index1')\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         store_reduction = ops.store_reduction('buf1', get_index_1, getitem)\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         return store_reduction\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] \n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] \n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op2: SchedulerNode(ComputedBuffer)\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op2.writes = [MemoryDep('buf2', c0, {c0: 100})]\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op2.unmet_dependencies = [MemoryDep('buf0', c0, {c0: 10000})]\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op2.met_dependencies = []\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op2.outputs = [\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     buf2: ComputedBuffer\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     buf2.layout = FixedLayout('cuda:0', torch.float32, size=[100, 1], stride=[1, 100])\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     buf2.users = [NodeUser(node=SchedulerNode(name='op3'), can_inplace=False, is_weak=False)]\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] ]\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op2.group.device = cuda:0\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op2.group.iteration = (100, 100)\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op2.sizes = ([100], [100])\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] buf0_layout = FixedLayout('cuda:0', torch.float32, size=[100, 100], stride=[100, 1])\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] buf2_layout = FixedLayout('cuda:0', torch.float32, size=[100, 1], stride=[1, 100])\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] class op2_loop_body:\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     var_ranges = {p0: 100, p1: 100}\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     index0 = 100*p0 + p1\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     index1 = p0\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     def body(self, ops):\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         get_index = self.get_index('index0')\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         load = ops.load('buf0', get_index)\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         reduction = ops.reduction(torch.float32, torch.float32, 'online_softmax_reduce', load)\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         getitem = reduction[0]\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         getitem_1 = reduction[1]\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         get_index_1 = self.get_index('index1')\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         store_reduction = ops.store_reduction('buf2', get_index_1, getitem_1)\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         return store_reduction\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] \n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] \n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op3: SchedulerNode(ComputedBuffer)\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op3.writes = [MemoryDep('buf3', c0, {c0: 10000})]\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op3.unmet_dependencies = \n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     [   MemoryDep('buf0', c0, {c0: 10000}),\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         MemoryDep('buf1', c0, {c0: 100}),\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         MemoryDep('buf2', c0, {c0: 100})]\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op3.met_dependencies = []\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op3.outputs = [\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     buf3: ComputedBuffer\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     buf3.layout = FixedLayout('cuda:0', torch.float32, size=[100, 100], stride=[100, 1])\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     buf3.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] ]\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op3.group.device = cuda:0\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op3.group.iteration = (10000, 1)\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op3.sizes = ([100, 100], [])\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] buf0_layout = FixedLayout('cuda:0', torch.float32, size=[100, 100], stride=[100, 1])\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] buf1_layout = FixedLayout('cuda:0', torch.float32, size=[100, 1], stride=[1, 100])\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] buf2_layout = FixedLayout('cuda:0', torch.float32, size=[100, 1], stride=[1, 100])\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] buf3_layout = FixedLayout('cuda:0', torch.float32, size=[100, 100], stride=[100, 1])\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] class op3_loop_body:\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     var_ranges = {p0: 100, p1: 100}\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     index0 = 100*p0 + p1\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     index1 = p0\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     def body(self, ops):\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         get_index = self.get_index('index0')\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         load = ops.load('buf0', get_index)\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         get_index_1 = self.get_index('index1')\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         load_1 = ops.load('buf1', get_index_1)\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         sub = ops.sub(load, load_1)\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         exp = ops.exp(sub)\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         get_index_2 = self.get_index('index1')\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         load_2 = ops.load('buf2', get_index_2)\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         truediv = ops.truediv(exp, load_2)\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         get_index_3 = self.get_index('index0')\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         store = ops.store('buf3', get_index_3, truediv, None)\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         return store\n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] \n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] \n",
      "I0113 15:50:39.608000 550945 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] \n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] AFTER FUSION\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op0: ExternKernelSchedulerNode(ExternKernelOut)\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op0.writes = [StarDep(name='buf0', mode=None)]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op0.unmet_dependencies = []\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op0.met_dependencies = [StarDep(name='arg0_1', mode=None), StarDep(name='arg1_1', mode=None)]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op0.outputs = [\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf0: ExternKernelOut\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf0.layout = FixedLayout('cuda:0', torch.float32, size=[100, 100], stride=[100, 1])\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf0.users = [\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         NodeUser(node=SchedulerNode(name='op1'), can_inplace=False, is_weak=False),\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         NodeUser(node=SchedulerNode(name='op2'), can_inplace=False, is_weak=False),\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         NodeUser(node=SchedulerNode(name='op3'), can_inplace=True, is_weak=False),\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     ]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] ]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op0.node.kernel = extern_kernels.mm\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] \n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] \n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1_op2_op3: FusedSchedulerNode(SchedulerNode,SchedulerNode,SchedulerNode)\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1_op2_op3.writes = \n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     [   MemoryDep('buf1', c0, {c0: 100}),\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         MemoryDep('buf2', c0, {c0: 100}),\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         MemoryDep('buf3', c0, {c0: 10000})]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1_op2_op3.unmet_dependencies = [MemoryDep('buf0', c0, {c0: 10000})]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1_op2_op3.met_dependencies = []\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1_op2_op3.outputs = [\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf1: ComputedBuffer\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf1.layout = FixedLayout('cuda:0', torch.float32, size=[100, 1], stride=[1, 100])\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf1.users = [NodeUser(node=SchedulerNode(name='op3'), can_inplace=False, is_weak=False)]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf2: ComputedBuffer\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf2.layout = FixedLayout('cuda:0', torch.float32, size=[100, 1], stride=[1, 100])\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf2.users = [NodeUser(node=SchedulerNode(name='op3'), can_inplace=False, is_weak=False)]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf3: ComputedBuffer\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf3.layout = FixedLayout('cuda:0', torch.float32, size=[100, 100], stride=[100, 1])\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf3.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] ]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1_op2_op3.snodes[0] =\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1: SchedulerNode(ComputedBuffer)\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1.writes = [MemoryDep('buf1', c0, {c0: 100})]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1.unmet_dependencies = [MemoryDep('buf0', c0, {c0: 10000})]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1.met_dependencies = []\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1.outputs = [\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf1: ComputedBuffer\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf1.layout = FixedLayout('cuda:0', torch.float32, size=[100, 1], stride=[1, 100])\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf1.users = [NodeUser(node=SchedulerNode(name='op3'), can_inplace=False, is_weak=False)]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] ]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1.group.device = cuda:0\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1.group.iteration = (100, 100)\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1.sizes = ([100], [100])\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] buf0_layout = FixedLayout('cuda:0', torch.float32, size=[100, 100], stride=[100, 1])\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] buf1_layout = FixedLayout('cuda:0', torch.float32, size=[100, 1], stride=[1, 100])\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] class op1_loop_body:\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     var_ranges = {p0: 100, p1: 100}\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     index0 = 100*p0 + p1\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     index1 = p0\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     def body(self, ops):\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         get_index = self.get_index('index0')\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         load = ops.load('buf0', get_index)\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         reduction = ops.reduction(torch.float32, torch.float32, 'online_softmax_reduce', load)\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         getitem = reduction[0]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         getitem_1 = reduction[1]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         get_index_1 = self.get_index('index1')\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         store_reduction = ops.store_reduction('buf1', get_index_1, getitem)\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         return store_reduction\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1_op2_op3.snodes[1] =\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op2: SchedulerNode(ComputedBuffer)\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op2.writes = [MemoryDep('buf2', c0, {c0: 100})]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op2.unmet_dependencies = [MemoryDep('buf0', c0, {c0: 10000})]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op2.met_dependencies = []\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op2.outputs = [\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf2: ComputedBuffer\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf2.layout = FixedLayout('cuda:0', torch.float32, size=[100, 1], stride=[1, 100])\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf2.users = [NodeUser(node=SchedulerNode(name='op3'), can_inplace=False, is_weak=False)]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] ]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op2.group.device = cuda:0\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op2.group.iteration = (100, 100)\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op2.sizes = ([100], [100])\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] buf0_layout = FixedLayout('cuda:0', torch.float32, size=[100, 100], stride=[100, 1])\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] buf2_layout = FixedLayout('cuda:0', torch.float32, size=[100, 1], stride=[1, 100])\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] class op2_loop_body:\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     var_ranges = {p0: 100, p1: 100}\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     index0 = 100*p0 + p1\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     index1 = p0\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     def body(self, ops):\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         get_index = self.get_index('index0')\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         load = ops.load('buf0', get_index)\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         reduction = ops.reduction(torch.float32, torch.float32, 'online_softmax_reduce', load)\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         getitem = reduction[0]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         getitem_1 = reduction[1]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         get_index_1 = self.get_index('index1')\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         store_reduction = ops.store_reduction('buf2', get_index_1, getitem_1)\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         return store_reduction\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1_op2_op3.snodes[2] =\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op3: SchedulerNode(ComputedBuffer)\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op3.writes = [MemoryDep('buf3', c0, {c0: 10000})]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op3.unmet_dependencies = \n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     [   MemoryDep('buf0', c0, {c0: 10000}),\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         MemoryDep('buf1', c0, {c0: 100}),\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         MemoryDep('buf2', c0, {c0: 100})]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op3.met_dependencies = []\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op3.outputs = [\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf3: ComputedBuffer\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf3.layout = FixedLayout('cuda:0', torch.float32, size=[100, 100], stride=[100, 1])\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf3.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] ]\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op3.group.device = cuda:0\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op3.group.iteration = (10000, 1)\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op3.sizes = ([100, 100], [])\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] buf0_layout = FixedLayout('cuda:0', torch.float32, size=[100, 100], stride=[100, 1])\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] buf1_layout = FixedLayout('cuda:0', torch.float32, size=[100, 1], stride=[1, 100])\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] buf2_layout = FixedLayout('cuda:0', torch.float32, size=[100, 1], stride=[1, 100])\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] buf3_layout = FixedLayout('cuda:0', torch.float32, size=[100, 100], stride=[100, 1])\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] class op3_loop_body:\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     var_ranges = {p0: 100, p1: 100}\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     index0 = 100*p0 + p1\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     index1 = p0\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     def body(self, ops):\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         get_index = self.get_index('index0')\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         load = ops.load('buf0', get_index)\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         get_index_1 = self.get_index('index1')\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         load_1 = ops.load('buf1', get_index_1)\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         sub = ops.sub(load, load_1)\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         exp = ops.exp(sub)\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         get_index_2 = self.get_index('index1')\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         load_2 = ops.load('buf2', get_index_2)\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         truediv = ops.truediv(exp, load_2)\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         get_index_3 = self.get_index('index0')\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         store = ops.store('buf3', get_index_3, truediv, None)\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         return store\n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] \n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] \n",
      "I0113 15:50:39.617000 550945 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] \n",
      "tensor([[3.3965e-02, 7.1606e-03, 6.4530e-04,  ..., 8.3629e-04, 1.3002e-02,\n",
      "         9.0926e-05],\n",
      "        [5.5696e-02, 8.6700e-03, 8.5205e-04,  ..., 5.5737e-04, 1.6863e-02,\n",
      "         1.9311e-04],\n",
      "        [1.6127e-01, 1.0290e-02, 4.1131e-04,  ..., 5.4272e-03, 7.6076e-02,\n",
      "         1.7427e-04],\n",
      "        ...,\n",
      "        [1.0837e-01, 9.7398e-03, 6.4892e-04,  ..., 3.1884e-04, 9.3733e-03,\n",
      "         3.3966e-05],\n",
      "        [1.3028e-01, 1.5538e-02, 9.5697e-04,  ..., 1.1668e-03, 2.9701e-03,\n",
      "         1.0276e-04],\n",
      "        [3.7169e-02, 2.3652e-03, 3.4091e-03,  ..., 7.2790e-04, 1.1711e-02,\n",
      "         2.4589e-04]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "!TORCH_LOGS=\"ir_pre_fusion,ir_post_fusion\" TORCHINDUCTOR_FORCE_DISABLE_CACHES=1 python torch_compile_builtin_fusion.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934dc2a0-e154-4e77-ab2d-b330c550ab62",
   "metadata": {},
   "source": [
    "Let's have a more customized toy code `torch_compile_custom_toy.py` with branches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89f6bdca-81fc-461e-8cf3-1029cdd2315f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.1075, -0.3664,  0.2113, -0.5729,  0.0149,  0.3904, -0.3129,  1.0964,\n",
      "         0.1867, -0.4614])\n",
      "tensor([ 0.0096, -0.2170,  0.3665, -0.0233,  0.2757,  0.0350,  0.0531,  0.3920,\n",
      "         0.0509,  0.6125])\n",
      "tensor([ 0.3135, -0.4333, -0.2436, -0.5342, -0.0245, -0.4712,  0.0111,  0.1872,\n",
      "        -0.1006, -0.1696])\n",
      "tensor([-0.3448, -0.0287, -1.2579, -0.4756, -0.3052, -0.1545, -1.0011, -0.0348,\n",
      "        -0.2332, -0.2360])\n",
      "tensor([-0.3706, -0.1271, -0.1108, -0.4871,  0.1010, -0.3885,  0.1082,  0.0667,\n",
      "         0.0929,  0.3780])\n",
      "tensor([-0.5566,  0.0852,  0.6850, -0.1277,  0.1957, -0.1823, -0.5636, -0.2572,\n",
      "        -0.4672,  0.0193])\n",
      "tensor([ 0.0365,  0.0038,  0.2167,  0.2694, -0.8846,  0.0393, -0.0992, -0.1545,\n",
      "         0.1965,  0.2731])\n",
      "tensor([ 0.1262,  0.6381, -0.5660, -0.0135, -0.0962,  0.1142,  0.0935, -0.0147,\n",
      "        -0.1915,  0.7921])\n",
      "tensor([ 0.4005,  0.8971, -0.2554, -0.0670,  0.6740, -1.2135, -0.3693, -2.1402,\n",
      "         0.0676,  0.1542])\n",
      "tensor([ 0.1144,  0.0662,  0.8342,  0.0110,  0.1775, -0.2555, -0.5363,  0.4603,\n",
      "         0.1060,  0.0426])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "@torch.compile\n",
    "def toy_example(a, b):\n",
    "    x = a / (torch.abs(a) + 1)\n",
    "    if b.sum() < 0:\n",
    "        b = b * -1\n",
    "    return x * b\n",
    "\n",
    "for _ in range(10):\n",
    "    res = toy_example(torch.randn(10), torch.randn(10))\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc433d-f904-4d5a-a090-a2568620e2af",
   "metadata": {},
   "source": [
    "Now, use the `TORCHINDUCTOR_FORCE_DISABLE_CACHES` flag forces Pytorch to recompile each time, and setting the `TORCH_COMPILE_DEBUG` will generate a more comprehensive set of intermediate output files under `./torch_compile_debug`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd3fa594-304f-4527-928c-9aa63f8991b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tk/Desktop/jupyter/simp-intelligence/.pixi/envs/default/lib/python3.13/site-packages/torch/_dynamo/pgo.py:525: UserWarning: dynamo_pgo force disabled by torch._inductor.config.force_disable_caches\n",
      "  warn_once(\n",
      "W0113 15:50:50.544000 551025 site-packages/torch/_inductor/debug.py:449] [0/0] model__0_inference_0 debug trace: /home/tk/Desktop/jupyter/simp-intelligence/simp_intelligence/torch/torch_compile_debug/run_2026_01_13_15_50_45_140088-pid_551025/torchinductor/model__0_inference_0.0\n",
      "W0113 15:50:51.256000 551025 site-packages/torch/_inductor/debug.py:449] [1/0] model__1_inference_1 debug trace: /home/tk/Desktop/jupyter/simp-intelligence/simp_intelligence/torch/torch_compile_debug/run_2026_01_13_15_50_45_140088-pid_551025/torchinductor/model__1_inference_1.1\n",
      "tensor([ 0.0489,  0.1555,  0.0107, -0.2745,  0.0188,  0.5434,  0.0668,  0.0561,\n",
      "         0.4224, -0.2818])\n",
      "W0113 15:50:51.845000 551025 site-packages/torch/_inductor/debug.py:449] [2/0] model__2_inference_2 debug trace: /home/tk/Desktop/jupyter/simp-intelligence/simp_intelligence/torch/torch_compile_debug/run_2026_01_13_15_50_45_140088-pid_551025/torchinductor/model__2_inference_2.2\n",
      "tensor([-0.0043, -0.1765, -0.1555, -0.1065, -0.7348,  0.0440,  0.4652, -0.0843,\n",
      "         0.1626,  0.1308])\n",
      "tensor([-0.4548,  0.4312, -0.2387,  0.0792, -0.1075,  0.0625,  0.0937,  1.0589,\n",
      "         0.8900, -0.0512])\n",
      "tensor([-0.1121,  0.2540, -0.0985, -0.4763, -0.0380,  0.4753,  0.1395, -0.2069,\n",
      "         0.0494, -0.2578])\n",
      "tensor([-1.1686e-03, -3.3716e-01, -1.1872e-01, -2.1263e-02,  5.7385e-01,\n",
      "         9.6563e-01, -3.0376e-01, -1.0912e+00,  1.2286e+00, -5.0984e-01])\n",
      "tensor([-0.1245, -0.3320, -0.0684, -0.1027, -1.3269, -0.3721,  0.7319, -0.4167,\n",
      "         0.2969,  0.6577])\n",
      "tensor([-0.0033,  0.4671, -0.0230,  0.6635,  0.2412,  0.3111,  0.0074,  0.4293,\n",
      "         0.0011, -0.5208])\n",
      "tensor([ 0.5220, -0.2157, -0.0473, -0.5869, -0.6473, -0.1378,  0.9594, -0.0922,\n",
      "         0.0124,  0.2681])\n",
      "tensor([-0.4248, -0.2618,  0.2200,  0.2729, -0.0601,  0.4113,  1.4172,  0.8440,\n",
      "        -0.3694, -0.0850])\n",
      "tensor([-0.3124,  0.0085,  0.3978,  0.2699,  0.5939, -0.0665, -0.0493, -0.0549,\n",
      "        -0.3094, -0.4017])\n",
      "aot_model___0_debug.log  aot_model___2_debug.log  model__1_inference_1.1\n",
      "aot_model___1_debug.log  model__0_inference_0.0   model__2_inference_2.2\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./torch_compile_debug\n",
    "!TORCH_COMPILE_DEBUG=1 TORCHINDUCTOR_FORCE_DISABLE_CACHES=1 python torch_compile_custom_toy.py\n",
    "!ls ./torch_compile_debug/run_*/torchinductor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3163b6-5dda-477c-b35b-1fe4a020915a",
   "metadata": {},
   "source": [
    "All the `model__X_inference_X.X` and `[X/0] [__ir_post_fusion]` names have $X=0,1,2$ representing the *basic blocks* of our toy code control flow.\n",
    "To lookup their original code positions FX graph code, take a look at, e.g., `model__2_inference_2.2/fx_graph_readable.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7e0052-3006-4953-90ff-4c3072356ffc",
   "metadata": {},
   "source": [
    "## CUDA Graph Support\n",
    "One can also generate CUDA graph using torch.compile \"reduce-overhead\" mode:\n",
    "```python\n",
    "optimized_model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "```\n",
    "However, PyTorch only reduces overhead for CUDA-only graphs which do not mutate inputs. It seems like PyTorch's support for this is limited [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c782c-fdcc-46e6-966d-21aaea22883b",
   "metadata": {},
   "source": [
    "## Custom Operators\n",
    "In PyTorch (2.4 or later), `torch.compile` supports *opaque callable* [2] custom operator (such as C++ kernels or Modular Mojo kernels), however, `torch.compile` is unable to trace into custom operators.\n",
    "\n",
    "Note that the graph-breaker must be wrapped around the PyTorch custom operator. If it mutates any input Tensors, their names must be specified.\n",
    "And if the operator returns anything, it must be registered as a FakeTensor kernel (aka meta kernel) to the custom operator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976e382c-5a5e-416f-b341-5ec2aa0c5d68",
   "metadata": {},
   "source": [
    "## Reference\n",
    "[1] https://docs.pytorch.org/docs/stable/generated/torch.compile.html\n",
    "\n",
    "[2] https://docs.pytorch.org/tutorials/advanced/python_custom_ops.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
