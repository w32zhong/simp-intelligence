{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20c11a7e-4125-4327-96b6-3fff1584d6a3",
   "metadata": {},
   "source": [
    "`torch.compile` is introduced in PyTorch 2.0 and is intended to replace TorchScript (`torch.jit`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d6a05-88ee-4cb1-ba1d-8e59a76b8952",
   "metadata": {},
   "source": [
    "## Overall\n",
    "A torch compiled Python code will go through two stages: TorchDynamo + Inductor:\n",
    "\n",
    "1. TorchDynamo\n",
    "Parse Python code and get Python byte code, then generate a \"FX Graph\".\n",
    "\n",
    "2. TorchInductor\n",
    "Convert FX Graph into efficient code with potential Operator Fusion.\n",
    "\n",
    "The higher-level FX Graph operators (e.g., aten.add) will be converted into a loop-level IR in which operation fusion will be performed and are merged into one loop (e.g., Add + ReLU).\n",
    "\n",
    "Then, it decides how to introduce hardware-dependent code into these loops (e.g., adding Tile size and `tl.program_id`).\n",
    "\n",
    "Finally, in codegen, these loops are processed with a Triton template engine to generate Python `@triton.jit` code (saved at `/tmp/torchinductor_xxx`).\n",
    "TorchInductor utilize Triton as a backend to further optimize the generated code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf860251-a54c-40a9-8498-50aaf2eeda25",
   "metadata": {},
   "source": [
    "## Example Dissect\n",
    "Create a toy example `torch_compile_builtin_fusion.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81365ba1-2894-43c4-9d68-4f7217c6c853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0042, 0.0006, 0.0176,  ..., 0.0003, 0.0017, 0.0257],\n",
      "        [0.0402, 0.0009, 0.0153,  ..., 0.0012, 0.0012, 0.0075],\n",
      "        [0.0427, 0.0011, 0.0052,  ..., 0.0008, 0.0016, 0.0079],\n",
      "        ...,\n",
      "        [0.0029, 0.0002, 0.0040,  ..., 0.0004, 0.0002, 0.0206],\n",
      "        [0.0013, 0.0015, 0.0185,  ..., 0.0024, 0.0001, 0.0043],\n",
      "        [0.0134, 0.0003, 0.0135,  ..., 0.0007, 0.0002, 0.0140]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand((100, 100), device='cuda')\n",
    "b = torch.rand((100, 100), device='cuda')\n",
    "\n",
    "def fn(x, y):\n",
    "    z = torch.matmul(x, y)\n",
    "    return torch.nn.functional.softmax(z, dim=1)\n",
    "\n",
    "compiled_fn = torch.compile(fn)\n",
    "print(compiled_fn(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc39e8f7-c90c-42f9-9163-716463c2bc39",
   "metadata": {},
   "source": [
    "We can print the two-stage outcome: `graph_code` (FX Graph code presentation), and `output_code` are the output Triton code by Inductor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7e0f32d-3765-41d4-a9c8-b020988d4c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code] TRACED GRAPH\n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]  ===== __compiled_fn_1_764aecdc_de0b_44f3_b87f_7dc1542901a0 =====\n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]  /home/tk/Desktop/jupyter/simp-intelligence/.pixi/envs/default/lib/python3.13/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]     def forward(self, L_y_: \"\u001b[31mf32\u001b[0m\u001b[34m[100, 100]\u001b[0m\u001b[2m\u001b[34m[100, 1]\u001b[0m\u001b[2m\u001b[32mcuda:0\u001b[0m\", L_x_: \"\u001b[31mf32\u001b[0m\u001b[34m[100, 100]\u001b[0m\u001b[2m\u001b[34m[100, 1]\u001b[0m\u001b[2m\u001b[32mcuda:0\u001b[0m\"):\n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         l_y_ = L_y_\n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         l_x_ = L_x_\n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         \n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]          \u001b[2m# File: /home/tk/Desktop/jupyter/simp-intelligence/simp_intelligence/torch/torch_compile_builtin_fusion.py:7 in fn, code: z = torch.matmul(x, y)\u001b[0m\n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         z: \"\u001b[31mf32\u001b[0m\u001b[34m[100, 100]\u001b[0m\u001b[2m\u001b[34m[100, 1]\u001b[0m\u001b[2m\u001b[32mcuda:0\u001b[0m\" = torch.matmul(l_x_, l_y_);  \u001b[2ml_x_ = l_y_ = None\u001b[0m\n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         \n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]          \u001b[2m# File: /home/tk/Desktop/jupyter/simp-intelligence/simp_intelligence/torch/torch_compile_builtin_fusion.py:8 in fn, code: return torch.nn.functional.softmax(z, dim=1)\u001b[0m\n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         softmax: \"\u001b[31mf32\u001b[0m\u001b[34m[100, 100]\u001b[0m\u001b[2m\u001b[34m[100, 1]\u001b[0m\u001b[2m\u001b[32mcuda:0\u001b[0m\" = torch.nn.functional.softmax(z, dim = \u001b[34m1\u001b[0m);  \u001b[2mz = None\u001b[0m\n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         return (softmax,)\n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         \n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] Output code: \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # AOT ID: ['0_inference']\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import torch\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import math\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import random\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import os\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import tempfile\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from math import inf, nan\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from cmath import nanj\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.utils import maybe_profile\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch import device, empty_strided\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import triton\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import triton.language as tl\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] aten = torch.ops.aten\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] inductor_ops = torch.ops.inductor\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] _quantized = torch.ops._quantized\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] async_compile = AsyncCompile()\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tk/ak/cakvrozgrag4reyjvpd7hik2cnclzuht4g7fcumnp2l43selmyqm.py\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # Topologically Sorted Source Nodes: [softmax], Original ATen: [aten._softmax]\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # Source node to ATen node mapping:\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] #   softmax => div\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # Graph fragment:\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] #   %prepare_softmax_online_default : [num_users=2] = call_function[target=torch.ops.prims.prepare_softmax_online.default](args = (%mm, 1), kwargs = {})\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] #   %sub_tensor : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%mm, %getitem), kwargs = {})\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] #   %exp_default : [num_users=1] = call_function[target=torch.ops.aten.exp.default](args = (%sub_tensor,), kwargs = {})\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] #   %div : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%exp_default, %getitem_1), kwargs = {})\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] triton_per_fused__softmax_0 = async_compile.triton('triton_per_fused__softmax_0', '''\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import triton\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import triton.language as tl\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] @triton_heuristics.persistent_reduction(\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     size_hints={'x': 128, 'r0_': 128},\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     reduction_hint=ReductionHint.INNER,\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     filename=__file__,\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     triton_meta={'signature': {'in_out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=28, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]]}]},\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__softmax_0', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 4, 'backend_hash': '7D055AB65A5FF60E1050B050060B47798E877F6A1CE347B72B5DF848DBA8494C', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 0, 'r0_': 120000}}\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] )\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] @triton.jit\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] def triton_per_fused__softmax_0(in_out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr):\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     xnumel = 100\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     r0_numel = 100\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     R0_BLOCK: tl.constexpr = 128\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     rnumel = r0_numel\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     RBLOCK: tl.constexpr = R0_BLOCK\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     xmask = xindex < xnumel\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     r0_index = tl.arange(0, R0_BLOCK)[None, :]\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     r0_offset = 0\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     r0_mask = r0_index < r0_numel\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     roffset = r0_offset\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     rindex = r0_index\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     r0_1 = r0_index\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     x0 = xindex\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp0 = tl.load(in_out_ptr0 + (r0_1 + 100*x0), r0_mask & xmask, other=0.0)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp3 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp5 = tl.where(r0_mask & xmask, tmp3, float(\"-inf\"))\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp6 = triton_helpers.max2(tmp5, 1)[:, None]\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp7 = tmp1 - tmp6\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp8 = tl_math.exp(tmp7)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp9 = tl.broadcast_to(tmp8, [XBLOCK, R0_BLOCK])\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp11 = tl.where(r0_mask & xmask, tmp9, 0)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp12 = tl.sum(tmp11, 1)[:, None]\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp13 = tmp0 - tmp6\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp14 = tl_math.exp(tmp13)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp15 = (tmp14 / tmp12)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tl.store(in_out_ptr0 + (r0_1 + 100*x0), tmp15, r0_mask & xmask)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] ''', device_str='cuda')\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] async_compile.wait(globals())\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] del async_compile\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] def call(args):\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     arg0_1, arg1_1 = args\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     args.clear()\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     assert_size_stride(arg0_1, (100, 100), (100, 1))\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     assert_size_stride(arg1_1, (100, 100), (100, 1))\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     with torch.cuda._DeviceGuard(0):\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         torch.cuda.set_device(0)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         buf0 = empty_strided_cuda((100, 100), (100, 1), torch.float32)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         # Topologically Sorted Source Nodes: [z], Original ATen: [aten.mm]\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         extern_kernels.mm(arg1_1, arg0_1, out=buf0)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         del arg0_1\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         del arg1_1\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         buf3 = buf0; del buf0  # reuse\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         # Topologically Sorted Source Nodes: [softmax], Original ATen: [aten._softmax]\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         stream0 = get_raw_stream(0)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         triton_per_fused__softmax_0.run(buf3, 100, 100, stream=stream0)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     return (buf3, )\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     from torch._inductor.utils import print_performance\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     arg0_1 = rand_strided((100, 100), (100, 1), device='cuda:0', dtype=torch.float32)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     arg1_1 = rand_strided((100, 100), (100, 1), device='cuda:0', dtype=torch.float32)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     fn = lambda: call([arg0_1, arg1_1])\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] if __name__ == \"__main__\":\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.083000 541798 site-packages/torch/_inductor/codecache.py:1237] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tk/4s/c4sezo6ijxma66v5wddsot6hferhctvtsdh3sv6ruk5urg3qz7iu.py\n",
      "tensor([[1.9636e-04, 5.5059e-03, 2.9583e-02,  ..., 1.7367e-03, 5.9918e-04,\n",
      "         4.8986e-03],\n",
      "        [4.2851e-04, 1.7208e-03, 4.2615e-03,  ..., 8.6686e-03, 2.4552e-03,\n",
      "         1.4911e-03],\n",
      "        [6.7801e-04, 1.0233e-03, 6.0441e-03,  ..., 4.1758e-03, 6.5886e-04,\n",
      "         6.9890e-03],\n",
      "        ...,\n",
      "        [4.5841e-05, 3.7691e-03, 2.8522e-02,  ..., 9.2559e-04, 1.2366e-03,\n",
      "         6.4949e-03],\n",
      "        [1.1206e-03, 2.5253e-03, 8.9327e-03,  ..., 5.3293e-03, 4.6866e-03,\n",
      "         4.8835e-03],\n",
      "        [2.4315e-04, 4.2437e-03, 5.2506e-03,  ..., 4.2439e-03, 1.1069e-03,\n",
      "         4.9094e-03]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "!TORCH_LOGS=\"graph_code,output_code\" python torch_compile_builtin_fusion.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d075b1-025f-4982-806e-299cbf21f430",
   "metadata": {},
   "source": [
    "As shown above, the generated Triton code is using a fused function `triton_per_fused__softmax_0`.\n",
    "\n",
    "However, the above example calls built-in torch functions without too much room for `torch.compile` optimization (and many stages are not applied at all).\n",
    "Let's have a more customized toy code `torch_compile_custom_toy.py` so that we can also look more deeper into Inductor stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89f6bdca-81fc-461e-8cf3-1029cdd2315f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1044, -0.0396, -0.1876, -0.8878,  0.3451,  0.6469, -0.2108, -0.0597,\n",
      "         0.9823, -0.4830])\n",
      "tensor([ 0.0235,  0.3957, -0.9331,  0.1595,  0.0335, -0.0365, -0.0452, -0.1091,\n",
      "         0.3090, -0.0818])\n",
      "tensor([ 6.4036e-04, -3.9523e-02,  5.7321e-01,  5.8537e-01, -9.4209e-02,\n",
      "         4.9878e-01,  1.1121e+00, -1.8547e-01,  6.8314e-01,  1.6813e-02])\n",
      "tensor([ 0.1461,  0.2607, -0.2339, -0.1078, -1.2529, -0.1974, -0.9010, -0.1161,\n",
      "         0.2254,  0.0440])\n",
      "tensor([ 0.1684, -0.7341, -0.2091,  0.0287,  0.0245, -0.1058, -0.3187, -0.4545,\n",
      "        -1.2139,  0.4952])\n",
      "tensor([ 6.6728e-02, -9.9504e-01, -7.9639e-01, -7.3802e-03,  7.2258e-01,\n",
      "        -4.4539e-01, -7.6488e-01,  2.3857e-04, -2.2679e-01, -1.9297e-01])\n",
      "tensor([ 0.2817,  0.7707, -0.3862, -0.1633, -0.7569,  0.0060, -0.0264,  0.2699,\n",
      "        -0.0318, -0.3182])\n",
      "tensor([-0.5252, -0.1214, -1.1576,  0.6829, -0.0219,  0.1326, -0.0383,  0.0491,\n",
      "         0.8157,  0.0070])\n",
      "tensor([ 0.5871, -0.2340, -0.5601, -0.7857, -0.5513, -0.1814, -0.0278, -0.0149,\n",
      "         0.4205, -0.0419])\n",
      "tensor([-4.3212e-01, -1.0037e+00,  2.7488e-01,  1.0554e-01, -1.0564e+00,\n",
      "        -5.1531e-01,  4.9761e-01, -8.9576e-04, -1.4305e-01,  4.6546e-02])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "@torch.compile\n",
    "def toy_example(a, b):\n",
    "    x = a / (torch.abs(a) + 1)\n",
    "    if b.sum() < 0:\n",
    "        b = b * -1\n",
    "    return x * b\n",
    "\n",
    "for _ in range(10):\n",
    "    res = toy_example(torch.randn(10), torch.randn(10))\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc433d-f904-4d5a-a090-a2568620e2af",
   "metadata": {},
   "source": [
    "The `TORCHINDUCTOR_FORCE_DISABLE_CACHES` flag forces Pytorch to recompile each time, and setting the `TORCH_COMPILE_DEBUG` will generate a more comprehensive set of intermediate output files under `./torch_compile_debug`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd3fa594-304f-4527-928c-9aa63f8991b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tk/Desktop/jupyter/simp-intelligence/.pixi/envs/default/lib/python3.13/site-packages/torch/_dynamo/pgo.py:525: UserWarning: dynamo_pgo force disabled by torch._inductor.config.force_disable_caches\n",
      "  warn_once(\n",
      "W0113 15:14:27.756000 544086 site-packages/torch/_inductor/debug.py:449] [0/0] model__0_inference_0 debug trace: /home/tk/Desktop/jupyter/simp-intelligence/simp_intelligence/torch/torch_compile_debug/run_2026_01_13_15_14_22_348694-pid_544086/torchinductor/model__0_inference_0.0\n",
      "W0113 15:14:28.478000 544086 site-packages/torch/_inductor/debug.py:449] [1/0] model__1_inference_1 debug trace: /home/tk/Desktop/jupyter/simp-intelligence/simp_intelligence/torch/torch_compile_debug/run_2026_01_13_15_14_22_348694-pid_544086/torchinductor/model__1_inference_1.1\n",
      "tensor([-0.5091,  0.1598, -0.3580,  0.1102, -0.4926,  0.3581,  0.0663, -0.0721,\n",
      "        -0.1218, -0.2372])\n",
      "W0113 15:14:29.068000 544086 site-packages/torch/_inductor/debug.py:449] [2/0] model__2_inference_2 debug trace: /home/tk/Desktop/jupyter/simp-intelligence/simp_intelligence/torch/torch_compile_debug/run_2026_01_13_15_14_22_348694-pid_544086/torchinductor/model__2_inference_2.2\n",
      "tensor([-0.0385,  0.0960, -0.0088, -1.0253, -0.0135, -0.0123, -0.6154, -0.2946,\n",
      "         0.3822,  0.0255])\n",
      "tensor([-0.1137,  0.3999,  0.0986, -0.3323,  0.2488,  2.0071,  0.2077, -0.8569,\n",
      "        -0.0041, -0.0911])\n",
      "tensor([-0.3047, -0.3986,  0.6356, -0.1243, -0.7249,  0.0057, -0.4718, -0.1681,\n",
      "        -0.0319,  0.0743])\n",
      "tensor([ 0.3965,  0.4743,  0.4068,  0.5544,  0.0956,  0.7439,  1.2570,  0.2293,\n",
      "         0.5599, -0.7617])\n",
      "tensor([ 0.0841,  0.3168,  0.0208, -0.0694, -0.1951,  0.1583, -0.0024, -0.1396,\n",
      "        -0.8891, -0.0429])\n",
      "tensor([ 0.1170,  0.3303,  0.1563, -0.2228, -0.1622,  0.4721,  0.4280,  0.3609,\n",
      "         0.3690,  0.4178])\n",
      "tensor([ 1.6880e+00,  7.6411e-02,  5.6717e-01, -1.2561e+00,  3.0336e-01,\n",
      "        -3.4909e-01,  2.9842e-01, -1.0857e-01,  6.5169e-04,  2.6055e-02])\n",
      "tensor([-0.5899,  0.4301, -0.0381, -0.4696, -0.0388, -0.4788, -0.1235,  0.6255,\n",
      "         0.0592,  0.1679])\n",
      "tensor([ 0.1551,  0.2888, -0.0348, -0.3541,  0.2756, -0.7786,  0.4922,  0.2927,\n",
      "        -0.3487,  0.2354])\n",
      "aot_model___0_debug.log  aot_model___2_debug.log  model__1_inference_1.1\n",
      "aot_model___1_debug.log  model__0_inference_0.0   model__2_inference_2.2\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./torch_compile_debug\n",
    "!TORCH_COMPILE_DEBUG=1 TORCHINDUCTOR_FORCE_DISABLE_CACHES=1 python torch_compile_custom_toy.py\n",
    "!ls ./torch_compile_debug/run_*/torchinductor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0165b4f-82ca-4c6c-b2ac-4a94b83a87f3",
   "metadata": {},
   "source": [
    "We can also pass in more specific flags to output the code before and after fusion taking palce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9da6a01-b6e5-49a8-8d40-bda942ee0a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tk/Desktop/jupyter/simp-intelligence/.pixi/envs/default/lib/python3.13/site-packages/torch/_dynamo/pgo.py:525: UserWarning: dynamo_pgo force disabled by torch._inductor.config.force_disable_caches\n",
      "  warn_once(\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] BEFORE FUSION\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op0: SchedulerNode(ComputedBuffer)\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op0.writes = [MemoryDep('buf0', 0, {})]\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op0.unmet_dependencies = []\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op0.met_dependencies = [MemoryDep('arg1_1', c0, {c0: 10})]\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op0.outputs = [\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     buf0: ComputedBuffer\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     buf0.layout = FixedLayout('cpu', torch.float32, size=[], stride=[])\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     buf0.users = [NodeUser(node=SchedulerNode(name='op2'), can_inplace=True, is_weak=False)]\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] ]\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op0.group.device = cpu\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op0.group.iteration = ((), (10,))\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op0.sizes = ([], [10])\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] arg1_1_layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] buf0_layout = FixedLayout('cpu', torch.float32, size=[], stride=[])\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] class op0_loop_body:\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     var_ranges = {p0: 10}\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     index0 = p0\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     index1 = 0\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     def body(self, ops):\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         get_index = self.get_index('index0')\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         load = ops.load('arg1_1', get_index)\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         reduction = ops.reduction(torch.float32, torch.float32, 'sum', load)\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         get_index_1 = self.get_index('index1')\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         store_reduction = ops.store_reduction('buf0', get_index_1, reduction)\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         return store_reduction\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] \n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] \n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op1: SchedulerNode(ComputedBuffer)\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op1.writes = [MemoryDep('buf1', c0, {c0: 10})]\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op1.unmet_dependencies = []\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op1.met_dependencies = [MemoryDep('arg0_1', c0, {c0: 10})]\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op1.outputs = [\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     buf1: ComputedBuffer\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     buf1.layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     buf1.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] ]\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op1.group.device = cpu\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op1.group.iteration = ((10,), ())\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op1.sizes = ([10], [])\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] arg0_1_layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] buf1_layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] class op1_loop_body:\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     var_ranges = {p0: 10}\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     index0 = p0\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     def body(self, ops):\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         get_index = self.get_index('index0')\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         load = ops.load('arg0_1', get_index)\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         get_index_1 = self.get_index('index0')\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         load_1 = ops.load('arg0_1', get_index_1)\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         abs_1 = ops.abs(load_1)\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         constant = ops.constant(1.0, torch.float32)\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         add = ops.add(abs_1, constant)\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         truediv = ops.truediv(load, add)\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         get_index_2 = self.get_index('index0')\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         store = ops.store('buf1', get_index_2, truediv, None)\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         return store\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] \n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] \n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op2: SchedulerNode(ComputedBuffer)\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op2.writes = [MemoryDep('buf2', 0, {})]\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op2.unmet_dependencies = [MemoryDep('buf0', 0, {})]\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op2.met_dependencies = []\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op2.outputs = [\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     buf2: ComputedBuffer\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     buf2.layout = FixedLayout('cpu', torch.bool, size=[], stride=[])\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     buf2.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] ]\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op2.group.device = cpu\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op2.group.iteration = ((), ())\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] op2.sizes = ([], [])\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] buf0_layout = FixedLayout('cpu', torch.float32, size=[], stride=[])\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] buf2_layout = FixedLayout('cpu', torch.bool, size=[], stride=[])\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] class op2_loop_body:\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     var_ranges = {}\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     index0 = 0\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]     def body(self, ops):\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         get_index = self.get_index('index0')\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         load = ops.load('buf0', get_index)\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         constant = ops.constant(0.0, torch.float32)\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         lt = ops.lt(load, constant)\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         get_index_1 = self.get_index('index0')\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         store = ops.store('buf2', get_index_1, lt, None)\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion]         return store\n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] \n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] \n",
      "I0113 15:17:07.579000 545412 site-packages/torch/_inductor/debug.py:672] [0/0] [__ir_pre_fusion] \n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] AFTER FUSION\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op0: SchedulerNode(ComputedBuffer)\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op0.writes = [MemoryDep('buf0', 0, {})]\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op0.unmet_dependencies = []\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op0.met_dependencies = [MemoryDep('arg1_1', c0, {c0: 10})]\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op0.outputs = [\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf0: ComputedBuffer\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf0.layout = FixedLayout('cpu', torch.float32, size=[], stride=[])\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf0.users = [NodeUser(node=SchedulerNode(name='op2'), can_inplace=True, is_weak=False)]\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] ]\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op0.group.device = cpu\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op0.group.iteration = ((), (10,))\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op0.sizes = ([], [10])\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] arg1_1_layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] buf0_layout = FixedLayout('cpu', torch.float32, size=[], stride=[])\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] class op0_loop_body:\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     var_ranges = {p0: 10}\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     index0 = p0\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     index1 = 0\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     def body(self, ops):\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         get_index = self.get_index('index0')\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         load = ops.load('arg1_1', get_index)\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         reduction = ops.reduction(torch.float32, torch.float32, 'sum', load)\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         get_index_1 = self.get_index('index1')\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         store_reduction = ops.store_reduction('buf0', get_index_1, reduction)\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         return store_reduction\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] \n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] \n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op2: SchedulerNode(ComputedBuffer)\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op2.writes = [MemoryDep('buf2', 0, {})]\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op2.unmet_dependencies = [MemoryDep('buf0', 0, {})]\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op2.met_dependencies = []\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op2.outputs = [\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf2: ComputedBuffer\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf2.layout = FixedLayout('cpu', torch.bool, size=[], stride=[])\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf2.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] ]\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op2.group.device = cpu\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op2.group.iteration = ((), ())\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op2.sizes = ([], [])\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] buf0_layout = FixedLayout('cpu', torch.float32, size=[], stride=[])\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] buf2_layout = FixedLayout('cpu', torch.bool, size=[], stride=[])\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] class op2_loop_body:\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     var_ranges = {}\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     index0 = 0\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     def body(self, ops):\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         get_index = self.get_index('index0')\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         load = ops.load('buf0', get_index)\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         constant = ops.constant(0.0, torch.float32)\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         lt = ops.lt(load, constant)\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         get_index_1 = self.get_index('index0')\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         store = ops.store('buf2', get_index_1, lt, None)\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         return store\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] \n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] \n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1: SchedulerNode(ComputedBuffer)\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1.writes = [MemoryDep('buf1', c0, {c0: 10})]\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1.unmet_dependencies = []\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1.met_dependencies = [MemoryDep('arg0_1', c0, {c0: 10})]\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1.outputs = [\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf1: ComputedBuffer\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf1.layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     buf1.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] ]\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1.group.device = cpu\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1.group.iteration = ((10,), ())\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] op1.sizes = ([10], [])\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] arg0_1_layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] buf1_layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] class op1_loop_body:\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     var_ranges = {p0: 10}\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     index0 = p0\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]     def body(self, ops):\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         get_index = self.get_index('index0')\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         load = ops.load('arg0_1', get_index)\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         get_index_1 = self.get_index('index0')\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         load_1 = ops.load('arg0_1', get_index_1)\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         abs_1 = ops.abs(load_1)\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         constant = ops.constant(1.0, torch.float32)\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         add = ops.add(abs_1, constant)\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         truediv = ops.truediv(load, add)\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         get_index_2 = self.get_index('index0')\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         store = ops.store('buf1', get_index_2, truediv, None)\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion]         return store\n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] \n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] \n",
      "I0113 15:17:07.589000 545412 site-packages/torch/_inductor/debug.py:679] [0/0] [__ir_post_fusion] \n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion] BEFORE FUSION\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion] op0: SchedulerNode(ComputedBuffer)\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion] op0.writes = [MemoryDep('buf0', c0, {c0: 10})]\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion] op0.unmet_dependencies = []\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion] op0.met_dependencies = [MemoryDep('arg0_1', c0, {c0: 10}), MemoryDep('arg1_1', c0, {c0: 10})]\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion] op0.outputs = [\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion]     buf0: ComputedBuffer\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion]     buf0.layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion]     buf0.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion] ]\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion] op0.group.device = cpu\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion] op0.group.iteration = ((10,), ())\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion] op0.sizes = ([10], [])\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion] arg0_1_layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion] arg1_1_layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion] buf0_layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion] class op0_loop_body:\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion]     var_ranges = {p0: 10}\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion]     index0 = p0\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion]     def body(self, ops):\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion]         get_index = self.get_index('index0')\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion]         load = ops.load('arg0_1', get_index)\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion]         get_index_1 = self.get_index('index0')\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion]         load_1 = ops.load('arg1_1', get_index_1)\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion]         mul = ops.mul(load, load_1)\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion]         get_index_2 = self.get_index('index0')\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion]         store = ops.store('buf0', get_index_2, mul, None)\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion]         return store\n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion] \n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion] \n",
      "I0113 15:17:12.843000 545412 site-packages/torch/_inductor/debug.py:672] [1/0] [__ir_pre_fusion] \n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion] AFTER FUSION\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion] op0: SchedulerNode(ComputedBuffer)\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion] op0.writes = [MemoryDep('buf0', c0, {c0: 10})]\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion] op0.unmet_dependencies = []\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion] op0.met_dependencies = [MemoryDep('arg0_1', c0, {c0: 10}), MemoryDep('arg1_1', c0, {c0: 10})]\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion] op0.outputs = [\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion]     buf0: ComputedBuffer\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion]     buf0.layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion]     buf0.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion] ]\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion] op0.group.device = cpu\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion] op0.group.iteration = ((10,), ())\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion] op0.sizes = ([10], [])\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion] arg0_1_layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion] arg1_1_layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion] buf0_layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion] class op0_loop_body:\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion]     var_ranges = {p0: 10}\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion]     index0 = p0\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion]     def body(self, ops):\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion]         get_index = self.get_index('index0')\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion]         load = ops.load('arg0_1', get_index)\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion]         get_index_1 = self.get_index('index0')\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion]         load_1 = ops.load('arg1_1', get_index_1)\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion]         mul = ops.mul(load, load_1)\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion]         get_index_2 = self.get_index('index0')\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion]         store = ops.store('buf0', get_index_2, mul, None)\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion]         return store\n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion] \n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion] \n",
      "I0113 15:17:12.846000 545412 site-packages/torch/_inductor/debug.py:679] [1/0] [__ir_post_fusion] \n",
      "tensor([-0.0995, -0.0114, -0.0599, -0.0613,  0.3313, -0.0140, -0.0308, -0.0321,\n",
      "        -0.5885, -0.3498])\n",
      "tensor([-0.0150, -0.3270,  0.2933,  0.2085, -0.3845, -1.1818,  0.3329, -0.5671,\n",
      "        -0.0758,  0.1473])\n",
      "tensor([ 0.3219, -0.3416, -0.2772,  0.1846,  0.2210, -0.3477, -0.6163, -0.0469,\n",
      "        -0.0381, -0.6846])\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion] BEFORE FUSION\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion] op0: SchedulerNode(ComputedBuffer)\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion] op0.writes = [MemoryDep('buf0', c0, {c0: 10})]\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion] op0.unmet_dependencies = []\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion] op0.met_dependencies = [MemoryDep('arg0_1', c0, {c0: 10}), MemoryDep('arg1_1', c0, {c0: 10})]\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion] op0.outputs = [\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion]     buf0: ComputedBuffer\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion]     buf0.layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion]     buf0.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion] ]\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion] op0.group.device = cpu\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion] op0.group.iteration = ((10,), ())\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion] op0.sizes = ([10], [])\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion] arg1_1_layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion] arg0_1_layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion] buf0_layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion] class op0_loop_body:\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion]     var_ranges = {p0: 10}\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion]     index0 = p0\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion]     def body(self, ops):\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion]         get_index = self.get_index('index0')\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion]         load = ops.load('arg1_1', get_index)\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion]         get_index_1 = self.get_index('index0')\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion]         load_1 = ops.load('arg0_1', get_index_1)\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion]         constant = ops.constant(-1.0, torch.float32)\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion]         mul = ops.mul(load_1, constant)\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion]         mul_1 = ops.mul(load, mul)\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion]         get_index_2 = self.get_index('index0')\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion]         store = ops.store('buf0', get_index_2, mul_1, None)\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion]         return store\n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion] \n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion] \n",
      "I0113 15:17:13.436000 545412 site-packages/torch/_inductor/debug.py:672] [2/0] [__ir_pre_fusion] \n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion] AFTER FUSION\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion] op0: SchedulerNode(ComputedBuffer)\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion] op0.writes = [MemoryDep('buf0', c0, {c0: 10})]\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion] op0.unmet_dependencies = []\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion] op0.met_dependencies = [MemoryDep('arg0_1', c0, {c0: 10}), MemoryDep('arg1_1', c0, {c0: 10})]\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion] op0.outputs = [\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion]     buf0: ComputedBuffer\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion]     buf0.layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion]     buf0.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion] ]\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion] op0.group.device = cpu\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion] op0.group.iteration = ((10,), ())\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion] op0.sizes = ([10], [])\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion] arg1_1_layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion] arg0_1_layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion] buf0_layout = FixedLayout('cpu', torch.float32, size=[10], stride=[1])\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion] class op0_loop_body:\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion]     var_ranges = {p0: 10}\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion]     index0 = p0\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion]     def body(self, ops):\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion]         get_index = self.get_index('index0')\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion]         load = ops.load('arg1_1', get_index)\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion]         get_index_1 = self.get_index('index0')\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion]         load_1 = ops.load('arg0_1', get_index_1)\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion]         constant = ops.constant(-1.0, torch.float32)\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion]         mul = ops.mul(load_1, constant)\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion]         mul_1 = ops.mul(load, mul)\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion]         get_index_2 = self.get_index('index0')\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion]         store = ops.store('buf0', get_index_2, mul_1, None)\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion]         return store\n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion] \n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion] \n",
      "I0113 15:17:13.439000 545412 site-packages/torch/_inductor/debug.py:679] [2/0] [__ir_post_fusion] \n",
      "tensor([-0.1908, -0.2555, -0.0116, -0.0470, -0.0631, -0.5087, -0.7987,  0.1749,\n",
      "        -0.2089, -0.0522])\n",
      "tensor([ 0.0484, -0.4930,  0.0219, -0.1650, -0.5104, -0.1844, -0.0285, -0.3906,\n",
      "         1.4323,  0.6188])\n",
      "tensor([ 0.2334,  0.8285, -1.1426,  0.3272,  0.1172, -0.2116, -0.0471, -0.1514,\n",
      "         0.1792,  0.0201])\n",
      "tensor([ 0.6688,  0.3707, -0.4519, -0.4033, -0.0050,  0.5341, -0.7988, -0.3525,\n",
      "         1.0211, -0.7188])\n",
      "tensor([-0.0980, -0.5440,  0.1949, -0.0027,  0.4817, -0.0008,  0.1019,  0.0110,\n",
      "        -0.5508,  0.6590])\n",
      "tensor([-0.3185,  0.1270,  0.2664, -0.0234,  0.4735,  0.1997,  0.2838, -0.7015,\n",
      "         1.1954, -0.4126])\n",
      "tensor([ 0.6312,  0.4748,  0.2263, -0.1214,  0.0600, -0.4527,  0.1614,  0.1798,\n",
      "         0.4895, -0.1931])\n"
     ]
    }
   ],
   "source": [
    "!TORCH_LOGS=\"ir_pre_fusion,ir_post_fusion\" TORCHINDUCTOR_FORCE_DISABLE_CACHES=1 python torch_compile_custom_toy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3163b6-5dda-477c-b35b-1fe4a020915a",
   "metadata": {},
   "source": [
    "All the `model__X_inference_X.X` and `[X/0] [__ir_post_fusion]` names ($X=0,1,2$) represent the *basic blocks* of our toy code control flow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
