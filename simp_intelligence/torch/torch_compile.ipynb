{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20c11a7e-4125-4327-96b6-3fff1584d6a3",
   "metadata": {},
   "source": [
    "`torch.compile` is introduced in PyTorch 2.0 and is intended to replace TorchScript (`torch.jit`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d6a05-88ee-4cb1-ba1d-8e59a76b8952",
   "metadata": {},
   "source": [
    "## Overall\n",
    "A torch compiled Python code will go through two stages: TorchDynamo + Inductor:\n",
    "\n",
    "1. TorchDynamo\n",
    "Parse Python code and get Python byte code, then generate a \"FX Graph\".\n",
    "\n",
    "2. TorchInductor\n",
    "Convert FX Graph into efficient code with potential Operator Fusion.\n",
    "\n",
    "The higher-level FX Graph operators (e.g., aten.add) will be converted into a loop-level IR in which operation fusion will be performed and are merged into one loop (e.g., Add + ReLU).\n",
    "\n",
    "Then, it decides how to introduce hardware-dependent code into these loops (e.g., adding Tile size and `tl.program_id`).\n",
    "\n",
    "Finally, in codegen, these loops are processed with a Triton template engine to generate Python `@triton.jit` code (saved at `/tmp/torchinductor_xxx`).\n",
    "TorchInductor utilize Triton as a backend to further optimize the generated code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf860251-a54c-40a9-8498-50aaf2eeda25",
   "metadata": {},
   "source": [
    "## Example Dissect\n",
    "Create a toy example `torch_compile_builtin_fusion.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81365ba1-2894-43c4-9d68-4f7217c6c853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0042, 0.0006, 0.0176,  ..., 0.0003, 0.0017, 0.0257],\n",
      "        [0.0402, 0.0009, 0.0153,  ..., 0.0012, 0.0012, 0.0075],\n",
      "        [0.0427, 0.0011, 0.0052,  ..., 0.0008, 0.0016, 0.0079],\n",
      "        ...,\n",
      "        [0.0029, 0.0002, 0.0040,  ..., 0.0004, 0.0002, 0.0206],\n",
      "        [0.0013, 0.0015, 0.0185,  ..., 0.0024, 0.0001, 0.0043],\n",
      "        [0.0134, 0.0003, 0.0135,  ..., 0.0007, 0.0002, 0.0140]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand((100, 100), device='cuda')\n",
    "b = torch.rand((100, 100), device='cuda')\n",
    "\n",
    "def fn(x, y):\n",
    "    z = torch.matmul(x, y)\n",
    "    return torch.nn.functional.softmax(z, dim=1)\n",
    "\n",
    "compiled_fn = torch.compile(fn)\n",
    "print(compiled_fn(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc39e8f7-c90c-42f9-9163-716463c2bc39",
   "metadata": {},
   "source": [
    "We can print the two-stage outcome: `graph_code` (FX Graph code presentation), and `output_code` are the output Triton code by Inductor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7e0f32d-3765-41d4-a9c8-b020988d4c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code] TRACED GRAPH\n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]  ===== __compiled_fn_1_764aecdc_de0b_44f3_b87f_7dc1542901a0 =====\n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]  /home/tk/Desktop/jupyter/simp-intelligence/.pixi/envs/default/lib/python3.13/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]     def forward(self, L_y_: \"\u001b[31mf32\u001b[0m\u001b[34m[100, 100]\u001b[0m\u001b[2m\u001b[34m[100, 1]\u001b[0m\u001b[2m\u001b[32mcuda:0\u001b[0m\", L_x_: \"\u001b[31mf32\u001b[0m\u001b[34m[100, 100]\u001b[0m\u001b[2m\u001b[34m[100, 1]\u001b[0m\u001b[2m\u001b[32mcuda:0\u001b[0m\"):\n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         l_y_ = L_y_\n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         l_x_ = L_x_\n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         \n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]          \u001b[2m# File: /home/tk/Desktop/jupyter/simp-intelligence/simp_intelligence/torch/torch_compile_builtin_fusion.py:7 in fn, code: z = torch.matmul(x, y)\u001b[0m\n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         z: \"\u001b[31mf32\u001b[0m\u001b[34m[100, 100]\u001b[0m\u001b[2m\u001b[34m[100, 1]\u001b[0m\u001b[2m\u001b[32mcuda:0\u001b[0m\" = torch.matmul(l_x_, l_y_);  \u001b[2ml_x_ = l_y_ = None\u001b[0m\n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         \n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]          \u001b[2m# File: /home/tk/Desktop/jupyter/simp-intelligence/simp_intelligence/torch/torch_compile_builtin_fusion.py:8 in fn, code: return torch.nn.functional.softmax(z, dim=1)\u001b[0m\n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         softmax: \"\u001b[31mf32\u001b[0m\u001b[34m[100, 100]\u001b[0m\u001b[2m\u001b[34m[100, 1]\u001b[0m\u001b[2m\u001b[32mcuda:0\u001b[0m\" = torch.nn.functional.softmax(z, dim = \u001b[34m1\u001b[0m);  \u001b[2mz = None\u001b[0m\n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         return (softmax,)\n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         \n",
      "V0113 15:02:14.896000 541798 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] Output code: \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # AOT ID: ['0_inference']\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import torch\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import math\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import random\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import os\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import tempfile\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from math import inf, nan\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from cmath import nanj\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.utils import maybe_profile\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch import device, empty_strided\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import triton\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import triton.language as tl\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] aten = torch.ops.aten\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] inductor_ops = torch.ops.inductor\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] _quantized = torch.ops._quantized\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] async_compile = AsyncCompile()\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tk/ak/cakvrozgrag4reyjvpd7hik2cnclzuht4g7fcumnp2l43selmyqm.py\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # Topologically Sorted Source Nodes: [softmax], Original ATen: [aten._softmax]\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # Source node to ATen node mapping:\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] #   softmax => div\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # Graph fragment:\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] #   %prepare_softmax_online_default : [num_users=2] = call_function[target=torch.ops.prims.prepare_softmax_online.default](args = (%mm, 1), kwargs = {})\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] #   %sub_tensor : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%mm, %getitem), kwargs = {})\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] #   %exp_default : [num_users=1] = call_function[target=torch.ops.aten.exp.default](args = (%sub_tensor,), kwargs = {})\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] #   %div : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%exp_default, %getitem_1), kwargs = {})\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] triton_per_fused__softmax_0 = async_compile.triton('triton_per_fused__softmax_0', '''\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import triton\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import triton.language as tl\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] @triton_heuristics.persistent_reduction(\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     size_hints={'x': 128, 'r0_': 128},\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     reduction_hint=ReductionHint.INNER,\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     filename=__file__,\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     triton_meta={'signature': {'in_out_ptr0': '*fp32', 'xnumel': 'i32', 'r0_numel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=28, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]]}]},\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_per_fused__softmax_0', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 4, 'backend_hash': '7D055AB65A5FF60E1050B050060B47798E877F6A1CE347B72B5DF848DBA8494C', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'tiling_scores': {'x': 0, 'r0_': 120000}}\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] )\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] @triton.jit\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] def triton_per_fused__softmax_0(in_out_ptr0, xnumel, r0_numel, XBLOCK : tl.constexpr):\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     xnumel = 100\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     r0_numel = 100\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     R0_BLOCK: tl.constexpr = 128\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     rnumel = r0_numel\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     RBLOCK: tl.constexpr = R0_BLOCK\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     xmask = xindex < xnumel\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     r0_index = tl.arange(0, R0_BLOCK)[None, :]\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     r0_offset = 0\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     r0_mask = r0_index < r0_numel\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     roffset = r0_offset\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     rindex = r0_index\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     r0_1 = r0_index\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     x0 = xindex\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp0 = tl.load(in_out_ptr0 + (r0_1 + 100*x0), r0_mask & xmask, other=0.0)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp1 = tl.broadcast_to(tmp0, [XBLOCK, R0_BLOCK])\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp3 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp5 = tl.where(r0_mask & xmask, tmp3, float(\"-inf\"))\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp6 = triton_helpers.max2(tmp5, 1)[:, None]\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp7 = tmp1 - tmp6\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp8 = tl_math.exp(tmp7)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp9 = tl.broadcast_to(tmp8, [XBLOCK, R0_BLOCK])\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp11 = tl.where(r0_mask & xmask, tmp9, 0)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp12 = tl.sum(tmp11, 1)[:, None]\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp13 = tmp0 - tmp6\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp14 = tl_math.exp(tmp13)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp15 = (tmp14 / tmp12)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tl.store(in_out_ptr0 + (r0_1 + 100*x0), tmp15, r0_mask & xmask)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] ''', device_str='cuda')\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] async_compile.wait(globals())\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] del async_compile\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] def call(args):\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     arg0_1, arg1_1 = args\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     args.clear()\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     assert_size_stride(arg0_1, (100, 100), (100, 1))\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     assert_size_stride(arg1_1, (100, 100), (100, 1))\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     with torch.cuda._DeviceGuard(0):\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         torch.cuda.set_device(0)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         buf0 = empty_strided_cuda((100, 100), (100, 1), torch.float32)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         # Topologically Sorted Source Nodes: [z], Original ATen: [aten.mm]\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         extern_kernels.mm(arg1_1, arg0_1, out=buf0)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         del arg0_1\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         del arg1_1\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         buf3 = buf0; del buf0  # reuse\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         # Topologically Sorted Source Nodes: [softmax], Original ATen: [aten._softmax]\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         stream0 = get_raw_stream(0)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         triton_per_fused__softmax_0.run(buf3, 100, 100, stream=stream0)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     return (buf3, )\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     from torch._inductor.utils import print_performance\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     arg0_1 = rand_strided((100, 100), (100, 1), device='cuda:0', dtype=torch.float32)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     arg1_1 = rand_strided((100, 100), (100, 1), device='cuda:0', dtype=torch.float32)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     fn = lambda: call([arg0_1, arg1_1])\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] if __name__ == \"__main__\":\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)\n",
      "V0113 15:02:16.081000 541798 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \n",
      "V0113 15:02:16.083000 541798 site-packages/torch/_inductor/codecache.py:1237] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tk/4s/c4sezo6ijxma66v5wddsot6hferhctvtsdh3sv6ruk5urg3qz7iu.py\n",
      "tensor([[1.9636e-04, 5.5059e-03, 2.9583e-02,  ..., 1.7367e-03, 5.9918e-04,\n",
      "         4.8986e-03],\n",
      "        [4.2851e-04, 1.7208e-03, 4.2615e-03,  ..., 8.6686e-03, 2.4552e-03,\n",
      "         1.4911e-03],\n",
      "        [6.7801e-04, 1.0233e-03, 6.0441e-03,  ..., 4.1758e-03, 6.5886e-04,\n",
      "         6.9890e-03],\n",
      "        ...,\n",
      "        [4.5841e-05, 3.7691e-03, 2.8522e-02,  ..., 9.2559e-04, 1.2366e-03,\n",
      "         6.4949e-03],\n",
      "        [1.1206e-03, 2.5253e-03, 8.9327e-03,  ..., 5.3293e-03, 4.6866e-03,\n",
      "         4.8835e-03],\n",
      "        [2.4315e-04, 4.2437e-03, 5.2506e-03,  ..., 4.2439e-03, 1.1069e-03,\n",
      "         4.9094e-03]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "!TORCH_LOGS=\"graph_code,output_code\" python torch_compile_builtin_fusion.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d075b1-025f-4982-806e-299cbf21f430",
   "metadata": {},
   "source": [
    "As shown above, the generated Triton code is using a fused function `triton_per_fused__softmax_0`.\n",
    "\n",
    "However, the above example calls built-in torch functions without too much room for `torch.compile` optimization (and many stages are not applied at all).\n",
    "Let's have a more customized toy code `torch_compile_custom_toy.py` so that we can also look more deeper into Inductor stages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd3fa594-304f-4527-928c-9aa63f8991b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2285,  0.6833, -0.0843,  0.1018, -0.4560,  1.2637,  0.0872,  0.1274,\n",
      "         0.0481,  0.3608])\n",
      "tensor([-0.0387, -0.0628, -0.2498,  0.5246, -0.1643,  0.0197,  0.1885, -0.0735,\n",
      "         0.2820,  1.0181])\n",
      "tensor([ 0.1763, -0.3590,  0.4897, -0.0080, -0.0753,  0.2756, -0.0122, -0.2288,\n",
      "         0.5179, -0.0016])\n",
      "tensor([-1.0154, -0.2763, -0.6568,  0.0159, -0.5978,  0.8283,  0.4505,  0.0409,\n",
      "        -0.3274, -0.8447])\n",
      "tensor([ 0.0906, -0.2199,  0.0134, -0.1255,  0.4967,  0.4357, -0.0527, -0.0063,\n",
      "         0.2549,  0.0277])\n",
      "tensor([-0.9464, -0.2473,  0.3010,  0.1190, -1.5808, -0.3638, -0.1464, -0.6412,\n",
      "         0.0051,  0.0452])\n",
      "tensor([-0.0885,  0.5260,  0.1119, -0.0861, -0.5349, -0.6390,  0.0381, -0.1691,\n",
      "         0.3603,  0.4051])\n",
      "tensor([-0.1284,  0.0015,  0.2658,  0.0079,  0.1679, -0.2159, -0.0157, -0.2208,\n",
      "         0.3690, -0.0501])\n",
      "tensor([-4.2978e-01, -7.7197e-02,  4.8542e-01,  6.5259e-01, -4.1098e-01,\n",
      "         2.3311e-04, -5.0730e-01, -2.9696e-01,  4.8463e-02, -3.8781e-01])\n",
      "tensor([ 0.0446,  0.0244,  0.0734,  0.2597,  0.1382,  0.3596, -0.5733, -1.5759,\n",
      "         0.2715,  0.0031])\n",
      "aot_model___0_debug.log\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./torch_compile_debug\n",
    "!TORCH_COMPILE_DEBUG=1 python torch_compile_custom_toy.py\n",
    "!ls ./torch_compile_debug/run_*/torchinductor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9da6a01-b6e5-49a8-8d40-bda942ee0a42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
