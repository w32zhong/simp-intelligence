{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea825c5a-cc27-4ea3-bd35-4d1edfaffc91",
   "metadata": {},
   "source": [
    "# Pytorch Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb7189e-f94d-468f-a984-db6580560f05",
   "metadata": {},
   "source": [
    "Let's start from an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef6162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True) # leaf node\n",
    "b = torch.sin(a)\n",
    "b.retain_grad() # We need to call retrain_grad() explicitly here for a non-leaf node, as we won't get non-leaf gradients by default.\n",
    "plt.plot(a.detach(), b.detach(), label=\"b\")\n",
    "\n",
    "c = torch.cos(a)\n",
    "c.retain_grad()\n",
    "plt.plot(a.detach(), c.detach(), label=\"c\")\n",
    "\n",
    "d = b + c\n",
    "d.retain_grad()\n",
    "plt.plot(a.detach(), d.detach(), label=\"d\")\n",
    "plt.legend()\n",
    "\n",
    "out = d.sum() # root node\n",
    "out.retain_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd603fb2-7b00-48c6-9097-d71002181217",
   "metadata": {},
   "source": [
    "The *computation graph* (a DAG) looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e44348b-3a68-4b91-8cee-8de778ec360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "dot = graphviz.Digraph()\n",
    "dot.node('a', 'a = [0, 2pi]')\n",
    "dot.node('b', 'b = sin(a)')\n",
    "dot.node('c', 'c = cos(a)')\n",
    "dot.node('d', 'd = b + c')\n",
    "dot.node('o', 'out = sum(d)')\n",
    "dot.edges(['ab', 'ac', 'bd', 'cd', 'do'])\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052e5f5f-cb56-4856-bec4-1f6f66f46037",
   "metadata": {},
   "source": [
    "## grad_fn, backward, and grad\n",
    "The `grad_fn` is the local function, and its property `next_functions` backtrack its parent node(s) in the computation graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b53e891-b6b9-4a37-8b1f-58b1f1056f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c7ffc1-595b-4ba7-b359-bf2eb0dcac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f5092b-0907-495b-acf9-69a20b0f611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that grad_fn `SinBackward0` is a built-in (non-python) function:\n",
    "import inspect\n",
    "inspect.getmro(b.grad_fn.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2709df7-26d2-4e83-a9e0-6a64a51fa1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2c725d-efc4-418b-b435-8616086f9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0264332e-180e-4238-94f1-574173b2f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5448ed9-754a-4f65-b1c0-85e2a200e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.grad_fn.next_functions # only a single parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c54e3-0d4b-49be-82a2-d793dd160cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.grad_fn.next_functions # two parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f851aa5-7413-4f3b-a00f-66ecb5aa086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.grad_fn.next_functions[1][0].next_functions # right parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c961eab9-3485-43d7-be56-bd67d23ba5dc",
   "metadata": {},
   "source": [
    "Before we get `grad`, we will need to \"back propagate\" (a typical implementation will go backward and accumulate gradients in reverese topological order):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28a7996-e0a1-4ce3-ac65-a2b52780e09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward(retain_graph=True) # retain_graph=True keep this computation graph for us to call backward() again (see later)\n",
    "# Also, in PyTorch, because out is a scalar, an implicit argument `gradient=tensor(1.)` is assumed.\n",
    "# This is convenient because most often it will be a loss value, and the derivatives of loss w.r.t. loss is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd52a9b8-7c85-4ec0-9e4a-391bf0d2d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.grad)\n",
    "print(b.grad)\n",
    "print(c.grad)\n",
    "print(d.grad)\n",
    "print(out.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c5a9ff-e4f2-4e89-9fc5-7d6328763e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a.detach(), b.detach(), label=\"b\")\n",
    "plt.plot(a.detach(), b.grad.detach(), label=\"b.grad\")\n",
    "plt.plot(a.detach(), a.grad.detach(), label=\"a.grad\")\n",
    "eps = 0.05\n",
    "plt.plot(a.detach(), torch.cos(a).detach() - torch.sin(a).detach() + eps, label=\"cos(a) - sin(a)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c781fa75-589b-4bbb-8d35-7081263896c7",
   "metadata": {},
   "source": [
    "if we think via math equations:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{out} &= \\sum_i d_i \\\\\n",
    "d_i &= b_i + c_i \\\\\n",
    "b_i &= \\sin(a_i) \\\\\n",
    "c_i &= \\cos(a_i) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "therefore\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{b[i].grad} &= \\frac{\\partial}{\\partial b_i}\\sum_{j}d_j = 1 \\\\\n",
    "\\text{a[i].grad} &=  1 \\cdot \\frac{\\partial}{\\partial a_i} (\\sin a_i + \\cos a_i) = \\cos a_i - \\sin a_i\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d8bee-1141-4882-a77a-ca289c65138f",
   "metadata": {},
   "source": [
    "Note that we can also call `backward()` from an intermediate node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f72c21-2e57-43df-baa3-2bde45e15022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the accumulated gradients\n",
    "a.grad.zero_()\n",
    "b.grad.zero_()\n",
    "c.grad.zero_()\n",
    "d.grad.zero_()\n",
    "out.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fe93dc-8a02-4ac8-b3cc-357921ef8efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.backward(gradient=torch.ones(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be238114-d3b7-4213-9454-72b61315ec08",
   "metadata": {},
   "source": [
    "We will get the same results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5629ce-d260-4d03-b860-c798920c5e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a.detach(), b.detach(), label=\"b\")\n",
    "plt.plot(a.detach(), b.grad.detach(), label=\"b.grad\")\n",
    "plt.plot(a.detach(), a.grad.detach(), label=\"a.grad\")\n",
    "eps = 0.05\n",
    "plt.plot(a.detach(), torch.cos(a).detach() - torch.sin(a).detach() + eps, label=\"cos(a) - sin(a)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34745cd7-e891-4bf3-a351-beffa42d1783",
   "metadata": {},
   "source": [
    "However, there is a minor difference: Because `out` node does not gets propogated this time, so its gradients is still zero for the last `backward()` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de8400-07e2-4e6b-8c30-bd42c369010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.grad)\n",
    "print(b.grad)\n",
    "print(c.grad)\n",
    "print(d.grad)\n",
    "print(out.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e360d1a-9572-426d-9978-5159941e65db",
   "metadata": {},
   "source": [
    "## Finally, a simple \"torch.tensor.backward()\"\n",
    "To dive a little bit more, take a look at a toy Tensor class in [backward.py](./backward.py) to understand how the backward method works in a nutshell.\n",
    "\n",
    "Note that, in real-world and production implementation (like in PyTorch), the \"DFS postorder\" topologicial ordering will be replaced with \"Kahnâ€™s algorithm\" because the latter produces deterministic ordering and the vertices can be executed in batch immediately when they are ready. Furthermore, the Kahn's algorithm naturally includes cycle detection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
