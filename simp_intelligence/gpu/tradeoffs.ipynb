{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f08568-18be-4c44-aa27-5eccc0d5700d",
   "metadata": {},
   "source": [
    "## Memory and Compute Bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085155f8-446c-4c0a-a7b5-cc773941951e",
   "metadata": {},
   "source": [
    "![](./roofline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa812f-1be9-4b51-a048-810e0783a880",
   "metadata": {},
   "source": [
    "A particular kernel's compute intensity (FLOPs per Byte move) and hardware determines the upperbound performance (FLOPs per second)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231f6413-3cda-4532-a065-ebf02ddabccb",
   "metadata": {},
   "source": [
    "When compute intensity is smaller (before the \"ridge point\"), it is mainly bounded by memory speed (Byte per second):\n",
    "$$\n",
    "\\text{FLOP} / \\text{Second} = \\frac{\\text{FLOP}}{\\text{Byte}} \\times \\frac{\\text{Byte}}{\\text{Second}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5176ed48-24d6-4366-bc96-12c9e70526af",
   "metadata": {},
   "source": [
    "By improving algorithm and adding compute intensity (e.g. vectorization), we move over the ridge point and since that the main bound becomes compute speed: \n",
    "$$\n",
    "\\text{FLOP} / \\text{Second} = \\min(\\frac{\\text{FLOP}}{\\text{Byte}} \\times \\frac{\\text{Byte}}{\\text{Second}} , \\text{Peak} \\frac{\\text{FLOP}}{\\text{Byte}}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047443b2-2bc6-4aca-b96a-4f228dfccba5",
   "metadata": {},
   "source": [
    "## Program Parallism vs. Resource Occupancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4de55d-f6a7-47b7-95bb-b29007698e8b",
   "metadata": {},
   "source": [
    "Assume an instruction without load/store can execute 1 op per 2 cycles, and an instruction with load/store will go down to 1 op per 400 cycles.\n",
    "If the load or store instruction are independent and can be parallelized (such as vector element additions), obviously, it is not smart to queue these instructions and execute them in order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42dcd2a-28c6-4481-82ed-3c0f456e1854",
   "metadata": {},
   "source": [
    "Instead, we can exploit the **program parallism** and *hide the latency* by making 200 parallel execution units and parallel threads, so that we take 400 cycles to execute 200 concurrent instructions resulting in an 1 op per 2 cycle *throughputs*.\n",
    "Assume you wait a minimum of 2 cycles before checking results, you will find no difference between the load/store instruction and a normal instruction, thus we **\"hide\"** the \"latency\" from you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ce2a2-8d7a-4cff-9356-abf64f38c1c1",
   "metadata": {},
   "source": [
    "Of course, in reality, GPU has a limited number of SM cores and compute units, and algorithm is not always fully parallelizable. As a result, we may want to greedy issue as many Warps as possible?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9989f0b7-c84b-476f-8ab7-21a935d9be5a",
   "metadata": {},
   "source": [
    "The answer is NO! In a blog post by Modular [1], they try to obtain the SoTA NF4 Dequantization kernel results on a pretty old GPU (Tesla T4), and the single most important step they manage to get there is to **reduce** the block size from 1024 threads to 512 threads.\n",
    "\n",
    "As we know,\n",
    "* if we set block size = 1024, we will end up 1024 / 32 = 32 Warps in the scheduled SM (a block cannot be breaked into multiple SMs);\n",
    "* if we set block size = 512, we will end up 512 / 32 = 16 Warps instead.\n",
    "\n",
    "And using less Warps apparently violates our intuition to create more parallel Warps (GPU can do \"scoreboard\" scheduling [2], and execute parallelizable Warps when compute units are available and its data dependency is met). Why? Because your goal is actually to have more Warps total on the SM. Reducing the Warps per Block is just the counter-intuitive tactic to achieve that goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5720a8-9d49-43ae-8ded-3d4bca9ab2a6",
   "metadata": {},
   "source": [
    "The real culprit is **resource occupancy**: because Tesla T4 is an old GPU, it has a high register pressure with the given thread workload.\n",
    "\n",
    "Because registers are strictly private to each individual thread in a GPU, more threads a Warp has, more register space the Warp will occupy.\n",
    "As a result, a 32-Warp block will consumes more register file space than a 16-Warp block.\n",
    "And if the 32-Warp block uses too much resources (like in the Tesla T4 case), no more extra Warps will be allowed to join the execution on this particular SM (when this block of warps stall on memory, nothing else can fill the bubble to hide the latency)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f71a98-bdf7-448a-9977-1bba7b590188",
   "metadata": {},
   "source": [
    "Assume Tesla T4 allows maximum 64 Warps to be concurrently scheduled on an SM, our high-demanding 32-Warp -- in the extreme case -- prevent any extra Warps to join the execution, resulting in a 32/64 = 50% occupancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603a8535-91dc-4aec-82ac-71b2a5b6dd7f",
   "metadata": {},
   "source": [
    "In a heavy work load where all SM will be utilized anyway, a 50% occupancy is far worse than a higher occupancy achieved by lowering the block size to 16 Warps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeab0704-f832-4771-94c9-49509f9fc0ce",
   "metadata": {},
   "source": [
    "*Footnote*: \n",
    "1. Compared to the Tomasulo algorithm, scoreboard scheduling is more simple to implement in hardware, as it does not involve result-forwarding bus (instead, it simply writes results back to the register), register renaming (historically not used, but can be added), and branch parallism.\n",
    "2. What about the other extreme -- we set the block size to only 32 so that many small blocks can be stacked on a SM to occupy the resources? The answer is -- No, in additional to the synchronization convenience (e.g., barrier, shared register and memory) a block can offer to its threads, (the block size in number of warps) $\\times$ (the maximum blocks per SM) should be $\\ge$ (the maximum warps per SM) to achieve 100% occupancy. Having a block size of 32 is not likely going to achieve this by hardware design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac1dccd-d8e1-493f-af66-68f084a83c2e",
   "metadata": {},
   "source": [
    "## Reference\n",
    "[1] https://www.modular.com/blog/how-to-beat-unsloth-s-cuda-kernel-using-mojo-with-zero-gpu-experience\n",
    "\n",
    "[2] https://patents.google.com/patent/US7676657 Nvidia Pattern: *Across-thread out-of-order instruction dispatch in a multithreaded microprocessor*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
